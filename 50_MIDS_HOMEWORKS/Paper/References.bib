@inproceedings{CityLearn,
author = {V\'{a}zquez-Canteli, Jos\'{e} R. and K\"{a}mpf, J\'{e}r\^{o}me and Henze, Gregor and Nagy, Zoltan},
title = {CityLearn v1.0: An OpenAI Gym Environment for Demand Response with Deep Reinforcement Learning},
year = {2019},
isbn = {9781450370059},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3360322.3360998},
doi = {10.1145/3360322.3360998},
abstract = {Demand response has the potential of reducing peaks of electricity demand by about 20% in the US, where buildings represent roughly 70% of the total electricity demand. Buildings are dynamic systems in constant change (i.e. occupants' behavior, refurbishment measures), which are costly to model and difficult to coordinate with other urban energy systems. Reinforcement learning is an adaptive control algorithm that can control these urban energy systems relying on historical and real-time data instead of models. Plenty of research has been conducted in the use of reinforcement learning for demand response applications in the last few years. However, most experiments are difficult to replicate, and the lack of standardization makes the performance of different algorithms difficult, if not impossible, to compare. In this demo, we introduce a new framework, CityLearn, based on the OpenAI Gym Environment, which will allow researchers to implement, share, replicate, and compare their implementations of reinforcement learning for demand response applications more easily. The framework is open source and modular, which allows researchers to modify and customize it, e.g., by adding additional storage, generation, or energy-consuming systems.},
booktitle = {Proceedings of the 6th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {356–357},
numpages = {2},
keywords = {Building Energy Control, Smart Buildings, Smart Grid},
location = {New York, NY, USA},
series = {BuildSys '19}
}

% Transfer learning
@misc{zhu2021transfer,
      title={Transfer Learning in Deep Reinforcement Learning: A Survey}, 
      author={Zhuangdi Zhu and Kaixiang Lin and Jiayu Zhou},
      year={2021},
      eprint={2009.07888},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% Behavior Cloning

@INPROCEEDINGS{Sutton00policygradient,
    author = {Richard S. Sutton and David Mcallester and Satinder Singh and Yishay Mansour},
    title = {Policy gradient methods for reinforcement learning with function approximation},
    booktitle = {In Advances in Neural Information Processing Systems 12},
    year = {2000},
    pages = {1057--1063},
    publisher = {MIT Press}
}

@misc{jena2020augmenting,
      title={Augmenting GAIL with BC for sample efficient imitation learning}, 
      author={Rohit Jena and Changliu Liu and Katia Sycara},
      year={2020},
      eprint={2001.07798},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% MAML
@misc{finn2017modelagnostic,
      title={Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks}, 
      author={Chelsea Finn and Pieter Abbeel and Sergey Levine},
      year={2017},
      eprint={1703.03400},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{SGW,
title = {{Smart grid - Wikipedia}},
url = {https://en.wikipedia.org/wiki/Smart_grid},
urldate = {2021-03-22}
}
@misc{GPV,
title = {{Growth of photovoltaics - Wikipedia}},
url = {https://en.wikipedia.org/wiki/Growth_of_photovoltaics},
urldate = {2021-03-22}
}
@misc{IEA2020,
title = {{Snapshot 2020 - IEA-PVPS}},
url = {https://iea-pvps.org/snapshot-reports/snapshot-2020/},
urldate = {2021-03-22}
}
@phdthesis{Hanna2019,
author = {Hanna, Josiah Paul},
file = {::},
month = {dec},
school = {The University of Texas at Austin},
title = {{Data Efficient Reinforcement Learning with Off-policy and Simulated Data Committee}}
}
@misc{,
title = {{Data efficient reinforcement learning with off-policy and simulated data}},
url = {https://repositories.lib.utexas.edu/handle/2152/80700},
urldate = {2021-03-21}
}
@techreport{DeepMind2021,
abstract = {DeepMind Lab is a first-person 3D game platform designed for research and development of general artificial intelligence and machine learning systems. DeepMind Lab can be used to study how autonomous artificial agents may learn complex tasks in large, partially observed, and visually diverse worlds. DeepMind Lab has a simple and flexible API enabling creative task-designs and novel AI-designs to be explored and quickly iterated upon. It is powered by a fast and widely recognised game engine, and tailored for effective use by the research community.},
archivePrefix = {arXiv},
arxivId = {1612.03801v2},
author = {Beattie, Charles and Leibo, Joel Z and Teplyashin, Denis and Ward, Tom and Wainwright, Marcus and K{\"{u}}ttler, Heinrich and Lefrancq, Andrew and Green, Simon and Vald{\'{e}}s, V{\'{i}}ctor and Sadik, Amir and Schrittwieser, Julian and Anderson, Keith and York, Sarah and Cant, Max and Cain, Adam and Bolton, Adrian and Gaffney, Stephen and King, Helen and Hassabis, Demis and Legg, Shane and Petersen, Stig},
eprint = {1612.03801v2},
file = {::},
title = {{DeepMind Lab}},
year = {2016}
}
@article{Li2012,
abstract = {This paper presents an improved reinforcement learning method to minimize electricity costs on the premise of satisfying the power balance and generation limit of units in a microgrid with grid-connected mode. Firstly, the microgrid control requirements are analyzed and the objective function of optimal control for microgrid is proposed. Then, a state variable Average Electricity Price Trend which is used to express the most possible transitions of the system is developed so as to reduce the complexity and randomicity of the microgrid, and a multi-agent architecture including agents, state variables, action variables and reward function is formulated. Furthermore, dynamic hierarchical reinforcement learning, based on change rate of key state variable, is established to carry out optimal policy exploration. The analysis shows that the proposed method is beneficial to handle the problem of curse of dimensionality and speed up learning in the unknown large-scale world. Finally, the simulation results under JADE (Java Agent Development Framework) demonstrate the validity of the presented method in optimal control for a microgrid with grid-connected mode. {\textcopyright} 2012 ISA.},
author = {Li, Fu Dong and Wu, Min and He, Yong and Chen, Xin},
doi = {10.1016/j.isatra.2012.06.010},
file = {:C\:/Users/felbu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2012 - Optimal control in microgrid using multi-agent reinforcement learning.pdf:pdf},
issn = {00190578},
journal = {ISA Transactions},
keywords = {Distributed generation,MAXQ,Microgrid,Multi-agent system,Reinforcement learning},
number = {6},
pages = {743--751},
pmid = {22824135},
publisher = {ISA - Instrumentation, Systems, and Automation Society},
title = {{Optimal control in microgrid using multi-agent reinforcement learning}},
volume = {51},
year = {2012}
}
@misc{Glockner2021,
author = {Glockner, Cyrill},
title = {{Simulators: The Key Training Environment for Applied Deep Reinforcement Learning | by Cyrill Glockner | Towards Data Science}},
url = {https://towardsdatascience.com/simulators-the-key-training-environment-for-applied-deep-reinforcement-learning-9a54353f494f},
urldate = {2021-03-21}
}
@misc{OpenAIGym,
title = {{Open AI Gym}},
url = {https://gym.openai.com/},
urldate = {2021-03-21}
}
@article{Kang2019,
abstract = {Deep reinforcement learning provides a promising approach for vision-based control of real-world robots. However, the generalization of such models depends critically on the quantity and variety of data available for training. This data can be difficult to obtain for some types of robotic systems, such as fragile, small-scale quadrotors. Simulated rendering and physics can provide for much larger datasets, but such data is inherently of lower quality: many of the phenomena that make the real-world autonomous flight problem challenging, such as complex physics and air currents, are modeled poorly or not at all, and the systematic differences between simulation and the real world are typically impossible to eliminate. In this work, we investigate how data from both simulation and the real world can be combined in a hybrid deep reinforcement learning algorithm. Our method uses real-world data to learn about the dynamics of the system, and simulated data to learn a generalizable perception system that can enable the robot to avoid collisions using only a monocular camera. We demonstrate our approach on a real-world nano aerial vehicle collision avoidance task, showing that with only an hour of real-world data, the quadrotor can avoid collisions in new environments with various lighting conditions and geometry. Code, instructions for building the aerial vehicles, and videos of the experiments can be found at github.com/gkahn13/GtS},
archivePrefix = {arXiv},
arxivId = {1902.03701},
author = {Kang, Katie and Belkhale, Suneel and Kahn, Gregory and Abbeel, Pieter and Levine, Sergey},
eprint = {1902.03701},
file = {::},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
month = {feb},
pages = {6008--6014},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Generalization through Simulation: Integrating Simulated and Real Data into Deep Reinforcement Learning for Vision-Based Autonomous Flight}},
url = {http://arxiv.org/abs/1902.03701},
volume = {2019-May},
year = {2019}
}
@misc{Dhamankar2020,
author = {Dhamankar, G. and Vazquez-Canteli, J. and Nagy, Z.},
title = {{Benchmarking Multi-Agent Deep RL Algorithms on Building Energy Demand Coordination Task}},
url = {https://www.youtube.com/watch?v=eWlVPKIB23o},
urldate = {2021-03-21}
}
@misc{Kurte2020,
author = {Kurte, K. and Munk, J. and Amasyali, K. and Kotevska, O. and Smith, R. and Zandi, H.},
title = {{Electricity Pricing aware Deep Reinforcement Learning based Intelligent HVAC Control}},
url = {https://www.youtube.com/watch?v=T6s2xdECsrs},
urldate = {2021-03-21}
}
@misc{Christensen2020,
author = {Christensen, M.H. and Ernewein, C.},
title = {{Demand Response through Price-setting Multi-agent Reinforcement Learning}},
url = {https://www.youtube.com/watch?v=_f0L313_4xE},
urldate = {2021-03-21}
}
@misc{NATG,
title = {{North American power transmission grid - Wikipedia}},
url = {https://en.wikipedia.org/wiki/North_American_power_transmission_grid},
urldate = {2021-03-21}
}
@article{Xu2020,
abstract = {This paper proposes a novel framework for home energy management (HEM) based on reinforcement learning in achieving efficient home-based demand response (DR). The concerned hour-ahead energy consumption scheduling problem is duly formulated as a finite Markov decision process (FMDP) with discrete time steps. To tackle this problem, a data-driven method based on neural network (NN) and Q-learning algorithm is developed, which achieves superior performance on cost-effective schedules for HEM system. Specifically, real data of electricity price and solar photovoltaic (PV) generation are timely processed for uncertainty prediction by extreme learning machine (ELM) in the rolling time windows. The scheduling decisions of the household appliances and electric vehicles (EVs) can be subsequently obtained through the newly developed framework, of which the objective is dual, i.e., to minimize the electricity bill as well as the DR induced dissatisfaction. Simulations are performed on a residential house level with multiple home appliances, an EV and several PV panels. The test results demonstrate the effectiveness of the proposed data-driven based HEM framework.},
author = {Xu, Xu and Jia, Youwei and Xu, Yan and Xu, Zhao and Chai, Songjian and Lai, Chun Sing},
doi = {10.1109/TSG.2020.2971427},
issn = {19493061},
journal = {IEEE Transactions on Smart Grid},
keywords = {Q-learning algorithm,Reinforcement learning,data-driven method,demand response,finite Markov decision process,home energy management,neural network},
month = {jul},
number = {4},
pages = {3201--3211},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{A Multi-Agent Reinforcement Learning-Based Data-Driven Method for Home Energy Management}},
volume = {11},
year = {2020}
}
@article{Tsui2012,
abstract = {Demand response (DR) is very important in the future smart grid, aiming to encourage consumers to reduce their demand during peak load hours. However, if binary decision variables are needed to specify start-up time of a particular appliance, the resulting mixed integer combinatorial problem is in general difficult to solve. In this paper, we study a versatile convex programming (CP) DR optimization framework for the automatic load management of various household appliances in a smart home. In particular, an L1 regularization technique is proposed to deal with schedule-based appliances (SAs), for which their on/off statuses are governed by binary decision variables. By relaxing these variables from integer to continuous values, the problem is reformulated as a new CP problem with an additional L1 regularization term in the objective. This allows us to transform the original mixed integer problem into a standard CP problem. Its major advantage is that the overall DR optimization problem remains to be convex and therefore the solution can be found efficiently. Moreover, a wide variety of appliances with different characteristics can be flexibly incorporated. Simulation result shows that the energy scheduling of SAs and other appliances can be determined simultaneously using the proposed CP formulation. {\textcopyright} 2010-2012 IEEE.},
author = {Tsui, K. M. and Chan, S. C.},
doi = {10.1109/TSG.2012.2218835},
issn = {19493053},
journal = {IEEE Transactions on Smart Grid},
keywords = {Convex programming,L1 regularization,demand response,energy consumption scheduling,energy management,smart home},
number = {4},
pages = {1812--1821},
title = {{Demand response optimization for smart home scheduling under real-time pricing}},
volume = {3},
year = {2012}
}
@misc{Northeast-Blackout,
title = {{Northeast blackout of 2003 - Wikipedia}},
url = {https://en.wikipedia.org/wiki/Northeast_blackout_of_2003},
urldate = {2021-03-21}
}
@misc{RLEM2020,
title = {{First Int'l Workshop on Reinforcement Learning for Energy Management in Buildings & Cities (RLEM)}},
url = {https://rlem-workshop.net/#program},
urldate = {2021-03-21}
}
@article{Wang2016,
abstract = {Integrating residential photovoltaic (PV) power generation and energy storage systems into the Smart Grid is an effective way of reducing fossil fuel consumptions. This has become a particularly interesting problem with the introduction of dynamic electricity energy pricing, since consumers can use their PV-based energy generation and controllable energy storage devices for peak shaving on their power demand profile, thereby minimizing their electricity bill. A realistic electricity pricing function is considered with billing period of a month, comprising both an energy price component and a demand price component. Due to the characteristics of electricity price function and energy storage capacity limitation, the residential storage control algorithm should 1) utilize PV power generation and load power consumption predictions and 2) account for various energy loss components during system operation, including energy loss components due to rate capacity effect in the storage system and power dissipation of the power conversion circuitry. A near-optimal storage control algorithm is proposed accounting for these aspects. The near-optimal algorithm, which controls the charging/discharging of the storage system, is effectively implemented by solving a convex optimization problem at the beginning of each day with polynomial time complexity. For further improvement, the reinforcement learning technique is adopted to adaptively determine the residual energy in the storage system at the end of each day in a billing period.},
author = {Wang, Yanzhi and Lin, Xue and Pedram, Massoud},
doi = {10.1109/TSTE.2015.2467190},
issn = {19493029},
journal = {IEEE Transactions on Sustainable Energy},
keywords = {Energy storage,Optimal control,Photovoltaic,Reinforcement learning},
month = {jan},
number = {1},
pages = {77--86},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{A near-optimal model-based control algorithm for households equipped with residential photovoltaic power generation and energy storage systems}},
volume = {7},
year = {2016}
}
@article{Yousefian2016,
abstract = {In this paper, a novel framework for designing and implementing a coordinated wide-area controller architecture for improved power system dynamic stability is presented and tested. The algorithm is an optimal wide-area system-centric controller and observer based on a hybrid reinforcement learning and temporal difference framework. It allows the system to deal with major concerns of wide-area monitoring problem: delays in signal transmission, the uncertainty of the communication network, and data traffic. The main advantage of this design is its ability to learn from the past using eligibility traces and predict the optimal trajectory of cost function through temporal difference method. The control algorithm is evolved from adaptive critic design (ACD) and performed online at a finite horizon through backward and forward view. The ACD controller's training and testing are implemented on the Innovative Integration Picolo card integrated to TMS320C28335 processor. Results on a real experimental test bed using a real power system feeder shows that this architecture provides better stability compared with conventional schemes.},
author = {Yousefian, Reza and Kamalasadan, Sukumar},
doi = {10.1109/TIA.2015.2466622},
issn = {00939994},
journal = {IEEE Transactions on Industry Applications},
keywords = {Adaptive critic design (ACD),Eligibility trace (ET),Real-time simulation,Temporal difference learning,Wide-area control},
month = {jan},
number = {1},
pages = {395--406},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Design and real-time implementation of optimal power system wide-area system-centric controller based on temporal difference learning}},
volume = {52},
year = {2016}
}
@article{Mocanu2019,
abstract = {Unprecedented high volumes of data are becoming available with the growth of the advanced metering infrastructure. These are expected to benefit planning and operation of the future power systems and to help customers transition from a passive to an active role. In this paper, we explore for the first time in the smart grid context the benefits of using deep reinforcement learning, a hybrid type of methods that combines reinforcement learning with deep learning, to perform on-line optimization of schedules for building energy management systems. The learning procedure was explored using two methods, Deep Q-learning and deep policy gradient, both of which have been extended to perform multiple actions simultaneously. The proposed approach was validated on the large-scale Pecan Street Inc. database. This highly dimensional database includes information about photovoltaic power generation, electric vehicles and buildings appliances. Moreover, these on-line energy scheduling strategies could be used to provide real-time feedback to consumers to encourage more efficient use of electricity.},
archivePrefix = {arXiv},
arxivId = {1707.05878},
author = {Mocanu, Elena and Mocanu, Decebal Constantin and Nguyen, Phuong H. and Liotta, Antonio and Webber, Michael E. and Gibescu, Madeleine and Slootweg, J. G.},
doi = {10.1109/TSG.2018.2834219},
eprint = {1707.05878},
issn = {19493053},
journal = {IEEE Transactions on Smart Grid},
keywords = {Deep reinforcement learning,deep neural networks,demand response,smart grid,strategic optimization},
month = {jul},
number = {4},
pages = {3698--3708},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{On-Line Building Energy Optimization Using Deep Reinforcement Learning}},
volume = {10},
year = {2019}
}
@article{Ademoye2012,
abstract = {In this paper, decentralized synergetic controllers with varying parameters are developed to dampen oscillations in electric power systems via the excitation systems of the generators. Each generator is treated as a subsystem for which a synergetic controller is designed. Each subsystem is a dynamical system driven by a function that estimates the effect of the rest of the system. A particle swarm optimization (PSO) technique is employed to initialize the controllers' gains. Then, reinforcement learning (RL) is used to vary the gains obtained after implementing the PSO so as to adapt the system to various operating conditions. Simulation results for a two area power system indicate that this technique gives a better performance than synergetic fixed gains controllers, or conventional power system stabilizers. Simulation results are obtained using the power analysis toolbox (PAT). {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
author = {Ademoye, Taoridi and Feliachi, Ali},
doi = {10.1016/j.epsr.2011.11.024},
issn = {03787796},
journal = {Electric Power Systems Research},
keywords = {Particle swarm optimization,Reinforcement learning,Synergetic control},
month = {may},
pages = {34--40},
title = {{Reinforcement learning tuned decentralized synergetic control of power systems}},
volume = {86},
year = {2012}
}
@article{Kuznetsova2013,
abstract = {We consider a microgrid for energy distribution, with a local consumer, a renewable generator (wind turbine) and a storage facility (battery), connected to the external grid via a transformer. We propose a 2 steps-ahead reinforcement learning algorithm to plan the battery scheduling, which plays a key role in the achievement of the consumer goals. The underlying framework is one of multi-criteria decision-making by an individual consumer who has the goals of increasing the utilization rate of the battery during high electricity demand (so as to decrease the electricity purchase from the external grid) and increasing the utilization rate of the wind turbine for local use (so as to increase the consumer independence from the external grid). Predictions of available wind power feed the reinforcement learning algorithm for selecting the optimal battery scheduling actions. The embedded learning mechanism allows to enhance the consumer knowledge about the optimal actions for battery scheduling under different time-dependent environmental conditions. The developed framework gives the capability to intelligent consumers to learn the stochastic environment and make use of the experience to select optimal energy management actions. {\textcopyright} 2013 Elsevier Ltd.},
author = {Kuznetsova, Elizaveta and Li, Yan Fu and Ruiz, Carlos and Zio, Enrico and Ault, Graham and Bell, Keith},
doi = {10.1016/j.energy.2013.05.060},
issn = {03605442},
journal = {Energy},
keywords = {Markov chain model,Microgrids,Reinforcement learning,Sensitivity analysis,Smartgrids},
month = {sep},
pages = {133--146},
publisher = {Elsevier Ltd},
title = {{Reinforcement learning for microgrid energy management}},
volume = {59},
year = {2013}
}
@misc{EIA,
title = {{Frequently Asked Questions (FAQs) - U.S. Energy Information Administration (EIA)}},
url = {https://www.eia.gov/tools/faqs/faq.php?id=105&t=3},
urldate = {2021-03-21}
}
@misc{SmartGrid,
title = {{What is a Smart Grid? | Live Science}},
url = {https://www.livescience.com/41920-smart-grid.html},
urldate = {2021-03-21}
}
@article{Glavic2017,
abstract = {In this paper, we review past (including very recent) research considerations in using reinforcement learning (RL) to solve electric power system decision and control problems. The RL considerations are reviewed in terms of specific electric power system problems, type of control and RL method used. We also provide observations about past considerations based on a comprehensive review of available publications. The review reveals the RL is considered as viable solutions to many decision and control problems across different time scales and electric power system states. Furthermore, we analyse the perspectives of RL approaches in light of the emergence of new-generation, communications, and instrumentation technologies currently in use, or available for future use, in power systems. The perspectives are also analysed in terms of recent breakthroughs in RL algorithms (Safe RL, Deep RL and path integral control for RL) and other, not previously considered, problems for RL considerations (most notably restorative, emergency controls together with so-called system integrity protection schemes, fusion with existing robust controls, and combining preventive and emergency control).},
author = {Glavic, Mevludin and Fonteneau, Rapha{\"{e}}l and Ernst, Damien},
doi = {10.1016/j.ifacol.2017.08.1217},
file = {:C\:/Users/felbu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Glavic, Fonteneau, Ernst - 2017 - Reinforcement Learning for Electric Power System Decision and Control Past Considerations and Perspect.pdf:pdf},
issn = {24058963},
journal = {IFAC-PapersOnLine},
keywords = {Electric power system,control,decision,reinforcement learning},
month = {jul},
number = {1},
pages = {6918--6927},
publisher = {Elsevier B.V.},
title = {{Reinforcement Learning for Electric Power System Decision and Control: Past Considerations and Perspectives}},
volume = {50},
year = {2017}
}
@article{Hadidi2013,
abstract = {In this paper, the design of a network of real-time close-loop wide-area decentralized power system stabilizers (WD-PSSs) is investigated. In this approach, real-time wide-area measurement data are processed and utilized to design a set of stability agents based on a Reinforcement Learning (RL) method. Recent technological breakthroughs in wide-area measurement system (WAMS) make the use of the system-wide signals possible in designing power system controllers. The main design objectives of these controllers are to stabilize the system after severe disturbances and mitigate the oscillations afterward. The proposed stability agents are decentralized and autonomous. The proposed method extends the stability boundary of the system and achieves the above goals without losing any generator or load area and without any knowledge of the disturbances causing the response. This paper describes the developed framework and addresses different challenges in designing such a network. A case study is provided to illustrate and verify the performance and robustness of the proposed approach. {\textcopyright} 2012 IEEE.},
author = {Hadidi, Ramtin and Jeyasurya, Benjamin},
doi = {10.1109/TSG.2012.2235864},
issn = {19493053},
journal = {IEEE Transactions on Smart Grid},
keywords = {Power system stability,real-time control,reinforcement learning,transient stability,wide-area control,wide-area measurement},
number = {1},
pages = {489--497},
title = {{Reinforcement learning based real-time wide-area stabilizing control agents to enhance power system stability}},
volume = {4},
year = {2013}
}
@article{Si2021,
abstract = {Advanced metering infrastructure and bilateral communication technologies facilitate the development of the home energy management system in the smart home. In this paper, we propose an energy management strategy for controllable loads based on reinforcement learning (RL). First, based on the mathematical model, the Markov decision process of different types of home energy resources (HERs) is formulated. Then, two RL algorithms, i.e. deep Q-learning and deep deterministic policy gradient are utilized. Based on the living habits of the residents, the dependency modes for HERs are proposed and are integrated into the reinforcement learning algorithms. Through the case studies, it is verified that the proposed method can schedule HERs properly to satisfy the established dependency modes. The difference between the achieved result and the optimal solution is relatively small.},
author = {Si, Caomingzhe and Tao, Yuechuan and Qiu, Jing and Lai, Shuying and Zhao, Junhua},
doi = {10.1007/s13042-020-01266-5},
issn = {1868808X},
journal = {International Journal of Machine Learning and Cybernetics},
keywords = {Dependency modes,Home energy management system,Reinforcement learning,Smart home},
month = {jan},
pages = {1--17},
publisher = {Springer Science and Business Media Deutschland GmbH},
title = {{Deep reinforcement learning based home energy management system with devices operational dependencies}},
url = {https://link.springer.com/article/10.1007/s13042-020-01266-5},
year = {2021}
}
@misc{Kosovo,
title = {{Serbia, Kosovo power grid row delays European clocks | Reuters}},
url = {https://www.reuters.com/article/serbia-kosovo-energy/serbia-kosovo-power-grid-row-delays-european-clocks-idUSL5N1QP2FF},
urldate = {2021-03-21}
}
@article{Gemine2017,
abstract = {With the increasing share of renewable and distributed generation in electrical distribution systems, active network management (ANM) becomes a valuable option for a distribution system operator to operate his system in a secure and cost-effective way without relying solely on network reinforcement. ANM strategies are short-term policies that control the power injected by generators and/or taken off by loads in order to avoid congestion or voltage issues. While simple ANM strategies consist in curtailing temporary excess generation, more advanced strategies rather attempt to move the consumption of loads to anticipated periods of high renewable generation. However, such advanced strategies imply that the system operator has to solve large-scale optimal sequential decision-making problems under uncertainty. The problems are sequential for several reasons. For example, decisions taken at a given moment constrain the future decisions that can be taken, and decisions should be communicated to the actors of the system sufficiently in advance to grant them enough time for implementation. Uncertainty must be explicitly accounted for because neither demand nor generation can be accurately forecasted. We first formulate the ANM problem, which in addition to be sequential and uncertain, has a nonlinear nature stemming from the power flow equations and a discrete nature arising from the activation of power modulation signals. This ANM problem is then cast as a stochastic mixed-integer nonlinear program, as well as second-order cone and linear counterparts, for which we provide quantitative results using state of the art solvers and perform a sensitivity analysis over the size of the system, the amount of available flexibility, and the number of scenarios considered in the deterministic equivalent of the stochastic program. To foster further research on this problem, we make available at http://www.montefiore.ulg.ac.be/$\sim$anm/ three test beds based on distribution networks of 5, 33, and 77 buses. These test beds contain a simulator of the distribution system, with stochastic models for the generation and consumption devices, and callbacks to implement and test various ANM strategies.},
archivePrefix = {arXiv},
arxivId = {1405.2806},
author = {Gemine, Quentin and Ernst, Damien and Corn{\'{e}}lusse, Bertrand},
doi = {10.1007/s11081-016-9339-9},
eprint = {1405.2806},
issn = {15732924},
journal = {Optimization and Engineering},
keywords = {Active network management,Electrical distribution network,Flexibility services,Large system,Mixed integer non-Linear program,Optimal sequential decision-making under uncertain,Renewable energy},
month = {sep},
number = {3},
pages = {587--629},
publisher = {Springer New York LLC},
title = {{Active network management for electrical distribution systems: problem formulation, benchmark, and approximate solution}},
volume = {18},
year = {2017}
}
@misc{SGCE,
title = {{Synchronous grid of Continental Europe - Wikipedia}},
url = {https://en.wikipedia.org/wiki/Synchronous_grid_of_Continental_Europe},
urldate = {2021-03-21}
}
@misc{SmartGrid-EU,
title = {{SuperSmart Grid — Potsdam Institute for Climate Impact Research}},
url = {https://www.pik-potsdam.de/en/output/projects/projects-archive/supersmart-grid-1},
urldate = {2021-03-21}
}
@misc{NYSERDA,
title = {{Smart Grid - NYSERDA}},
url = {https://www.nyserda.ny.gov/Researchers-and-Policymakers/Smart-Grid},
urldate = {2021-03-21}
}
@misc{Lambert2020,
author = {Lambert, Nathan},
title = {{Constructing Axes for Reinforcement Learning Policy - Democratizing Automation}},
url = {https://robotic.substack.com/p/rl-policy},
urldate = {2021-03-20},
year = {2020}
}
@misc{California-Energy-Crisis,
title = {{2000–01 California electricity crisis - Wikipedia}},
url = {https://en.wikipedia.org/wiki/2000–01_California_electricity_crisis},
urldate = {2021-03-21}
}
@inproceedings{Vazquez-Canteli2019,
abstract = {Demand response has the potential of reducing peaks of electricity demand by about 20% in the US, where buildings represent roughly 70% of the total electricity demand. Buildings are dynamic systems in constant change (i.e. occupants' behavior, refurbishment measures), which are costly to model and difficult to coordinate with other urban energy systems. Reinforcement learning is an adaptive control algorithm that can control these urban energy systems relying on historical and real-time data instead of models. Plenty of research has been conducted in the use of reinforcement learning for demand response applications in the last few years. However, most experiments are difficult to replicate, and the lack of standardization makes the performance of different algorithms difficult, if not impossible, to compare. In this demo, we introduce a new framework, CityLearn, based on the OpenAI Gym Environment, which will allow researchers to implement, share, replicate, and compare their implementations of reinforcement learning for demand response applications more easily. The framework is open source and modular, which allows researchers to modify and customize it, e.g., by adding additional storage, generation, or energy-consuming systems.},
address = {New York, NY, USA},
author = {V{\'{a}}zquez-Canteli, Jos{\'{e}} R. and K{\"{a}}mpf, J{\'{e}}r{\^{o}}me and Henze, Gregor and Nagy, Zoltan},
booktitle = {BuildSys 2019 - Proceedings of the 6th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
doi = {10.1145/3360322.3360998},
isbn = {9781450370059},
keywords = {Building Energy Control,Smart Buildings,Smart Grid},
month = {nov},
pages = {356--357},
publisher = {Association for Computing Machinery, Inc},
title = {{CityLearn v1.0: An OpenAI gym environment for demand response with deep reinforcement learning}},
url = {https://dl.acm.org/doi/10.1145/3360322.3360998},
year = {2019}
}
@misc{SmartGrid-USA,
title = {{Unified National Smart Grid | Repower America}},
url = {https://web.archive.org/web/20081112091929/http://www.repoweramerica.org/elements/unified-national-smart-grid/},
urldate = {2021-03-21}
}
@inproceedings{Aittahar2015,
abstract = {This paper aims to design an algorithm dedicated to operational planning for microgrids in the challenging case where the scenarios of production and consumption are not known in advance. Using expert knowledge obtained from solving a family of linear programs, we build a learning set for training a decision-making agent. The empirical performances in terms of Levelized Energy Cost (LEC) of the obtained agent are compared to the expert performances obtained in the case where the scenarios are known in advance. Preliminary results are promising.},
author = {Aittahar, Samy and Fran{\c{c}}ois-Lavet, Vincent and Lodeweyckx, Stefan and Ernst, Damien and Fonteneau, Raphael},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-27430-0_1},
isbn = {9783319274294},
issn = {16113349},
keywords = {Imitative learning,Machine learning,Microgrids,Planning},
pages = {1--15},
publisher = {Springer Verlag},
title = {{Imitative learning for online planning in microgrids}},
volume = {9518},
year = {2015}
}
@techreport{pomerleau-2021,
abstract = {ALVINN (Autonomous Land Vehicle In a Neural Network) is a 3-layer back-propagation network designed for the task of road following. Currently ALVINN takes images from a camera and a laser range finder as input and produces as output the direction the vehicle should travel in order to follow the road. Training has been conducted using simulated road images. Successful tests on the Carnegie Mellon autonomous navigation test vehicle indicate that the network can effectively follow real roads under certain field conditions. The representation developed to perfOIm the task differs dramatically when the networlc is trained under various conditions, suggesting the possibility of a novel adaptive autonomous navigation system capable of tailoring its processing to the conditions at hand.},
author = {Pomerleau, Dean A},
file = {::},
title = {{ALVINN: AN AUTONOMOUS LAND VEHICLE IN A NEURAL NETWORK}}
}
@misc{smartlab2021,
title = {{A brief overview of Imitation Learning | by SmartLab AI | Medium}},
url = {https://smartlabai.medium.com/a-brief-overview-of-imitation-learning-8a8a75c44a9c},
urldate = {2021-03-24}
}
@article{rashidinejad2021,
abstract = {Offline (or batch) reinforcement learning (RL) algorithms seek to learn an optimal policy from a fixed dataset without active data collection. Based on the composition of the offline dataset, two main categories of methods are used: imitation learning which is suitable for expert datasets and vanilla offline RL which often requires uniform coverage datasets. From a practical standpoint, datasets often deviate from these two extremes and the exact data composition is usually unknown a priori. To bridge this gap, we present a new offline RL framework that smoothly interpolates between the two extremes of data composition, hence unifying imitation learning and vanilla offline RL. The new framework is centered around a weak version of the concentrability coefficient that measures the deviation from the behavior policy to the expert policy alone. Under this new framework, we further investigate the question on algorithm design: can one develop an algorithm that achieves a minimax optimal rate and also adapts to unknown data composition? To address this question, we consider a lower confidence bound (LCB) algorithm developed based on pessimism in the face of uncertainty in offline RL. We study finite-sample properties of LCB as well as information-theoretic limits in multi-armed bandits, contextual bandits, and Markov decision processes (MDPs). Our analysis reveals surprising facts about optimality rates. In particular, in all three settings, LCB achieves a faster rate of $1/N$ for nearly-expert datasets compared to the usual rate of $1/\sqrt{N}$ in offline RL, where $N$ is the number of samples in the batch dataset. In the case of contextual bandits with at least two contexts, we prove that LCB is adaptively optimal for the entire data composition range, achieving a smooth transition from imitation learning to offline RL. We further show that LCB is almost adaptively optimal in MDPs.},
archivePrefix = {arXiv},
arxivId = {2103.12021},
author = {Rashidinejad, Paria and Zhu, Banghua and Ma, Cong and Jiao, Jiantao and Russell, Stuart},
eprint = {2103.12021},
file = {::},
month = {mar},
title = {{Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism}},
url = {http://arxiv.org/abs/2103.12021},
year = {2021}
}
@techreport{booher2021,
abstract = {Reinforcement Learning approaches in robotics are difficult because of the lack of an easy to define reward function that is dense in time. This suggests the use of imitation learning as a way to meaningfully guide exploration in an intelligent manner. We present an algorithm that allows for successful imitation learning from suboptimal demonstrations by combining behavioral cloning approaches with pure reinforcement learning to accelerate learning from sparse reward functions in robotic domains with long time horizons. For code: click here},
author = {Booher, Jonathan},
title = {{BC+RL: Imitation Learning From Non-Optimal Demonstrations}}
}
@article{Dulac2019,
abstract = {Reinforcement learning (RL) has proven its worth in a series of artificial domains, and is beginning to show some successes in real-world scenarios. However, much of the research advances in RL are often hard to leverage in real-world systems due to a series of assumptions that are rarely satisfied in practice. We present a set of nine unique challenges that must be addressed to productionize RL to real world problems. For each of these challenges, we specify the exact meaning of the challenge, present some approaches from the literature, and specify some metrics for evaluating that challenge. An approach that addresses all nine challenges would be applicable to a large number of real world problems. We also present an example domain that has been modified to present these challenges as a testbed for practical RL research.},
archivePrefix = {arXiv},
arxivId = {1904.12901},
author = {Dulac-Arnold, Gabriel and Mankowitz, Daniel and Hester, Todd},
eprint = {1904.12901},
file = {:C\:/Users/felbu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dulac-Arnold, Mankowitz, Hester - 2019 - Challenges of Real-World Reinforcement Learning.pdf:pdf},
journal = {arXiv},
month = {apr},
publisher = {arXiv},
title = {{Challenges of Real-World Reinforcement Learning}},
url = {http://arxiv.org/abs/1904.12901},
year = {2019}
}
