\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2018

% ready for submission
% \usepackage{neurips_2018}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{bbold}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{soul}
% for writing algorithm table
\usepackage{algorithm,algcompatible,amsmath}

\DeclareMathOperator*{\argmax}{\arg\!\max}% https://tex.stackexchange.com/q/83169/5764
\algnewcommand\INPUT{\item[\textbf{Input:}]}%
\algnewcommand\OUTPUT{\item[\textbf{Output:}]}%

\parindent30pt

\title{Deep Reinforcement Learning for Power System Service Restoration}

\author{
  Team members: Felipe Buchbinder, Shota Takeshima, Young Kyung Kim
}

\begin{document}
\maketitle
\section*{Plan}
- Unmet Need: Improving model performance compared to original paper. 
\newline
- Hypothesis/Goal/Aim: Our goal is to improve model. We have few hypothesis. We believe through using more advanced techniques as auto-encoder and recurrent neural network we would able to improve original model. We also believe that through incorporating volatility into reward function will improve original model.
\newline
- Objectives with (short, one-sentence) justification:  We aim to use reinforcement learning to build a model that can assist actors managing a hot or cold water system to maintain the stability of power generation and distribution, especially for efficiency in cost. Through building the model we are expected to write paper and submit to the conference.
\newline
- Audience: researchers
\newline
- Style: Academic paper(planning to submit the paper to conference)


\section*{Abstract}

to be added, since we don't have results

\section{Introduction}

\subsection{Motivation and importance of the problem}

Electrical power grids are an essential, yet extremely complicated piece infrastructure of city life. Responsible for the generating electricity and delivering it to the final consumers, the sheer sizes of some these power grids are a testimony to the difficulty of managing them. The Synchronous Grid of Continental Europe ([@SGCE]), for example, provides electricity to over 400 million people in 24 countries. In North America, two power grids are responsible for providing energy to the entire region, with the exception of Texas, Qu√©bec and Alaska ([@NATG]). 

Managing such power grids is as complex as it is relevant. Unexpected fluctuations in supply and demand may lead to power shortages (such as the 2003 Northeast Blackout, that affected an estimated 55 million people in US and Canada ([@Northeast-Blackout])), or to other adverse effects (such as when clocks in Europe ran slow because Kosovo used more electricity than anticipated as a result of its conflict with Serbia in 2018 ([@Kosovo])). Human tampering is also another cause for concern, as seen in the California energy crisis, when electricity companies such as Enron deliberately created a shortage in electricity supply in order to inflate prices ([@California-Energy-Crisis])

Even when power grids are operating normally, there is still loss due to the complexities of managing such behemoths. In the U.S., for instance, as much as 5\% of all generated energy is lost before reaching the customers ([@EIA]), leading states like New York to plan spending US\$30 billion over the next decade in power grid maintenance alone ([@NYSERDA]). 

Smart grids have been proposed as a means to more efficiently manage power grids [@SmartGrid]. By implementing telemetry across the entire grid and employing algorithms that can seamlessly make adjust the generation, transmission and distribution of energy, experts argue we could reduce losses and environmental impact without significant impact to current electricity prices. To be precise, as far as residential and commercial buildings are concerned, estimates are that the CO2 emission of buildings can be reduced by 50-90% ([@RLEM2020]).

While plans have been proposed to substitute both the North American ([@SmartGrid-USA]) and the European ([@SmartGrid-EU]) current power systems by smart grids, actually implementing smart grids, however, has caveats and challenges of its own. Among the challenges, one that is of particular concern to Artificial Intelligence (AI) scholars is how to train the algorithm that will manage such grids. In this paper, we use Reinforcement Learning to train an agent to manage a small power grid generated by the City Learn simulator package ([@Vazquez-Canteli2019]). We use this as an instance to illustrate how training times may be too long to be practically feasible, thereby compromising the potential values of such agents in real-world applications. We further discuss alternatives to speed up the training process.

\section{Literature Review}
\subsection{General statement of the Reinforcement Learning problem}

Reinforcement Learning (RL) is one of the branches of Machine Learning (ML), alongside with Supervised and Unsupervised Learning. In RL, there is an agent and an environment. The agent observes certain features of the environment and acts on what he observes, taking some action. As a result of the action he takes, he receives a certain reward and changes the environment in some way.  He then observes certain features of this new environment, and takes a new action. As a result, he gets a new reward and the environment is changed again. Thus the agent takes a succession of \emph{actions} based on what it perceives as being the \emph{state} of the environment, and receives a \emph{reward} for each action. The goal of RL is to teach the agent how to act in order to maximize the present value of the expected the reward it earns. Two terms are important in this statement: "teach how to act" and "present value."

By "teach the agent how to act", we mean deciding which action to take for each observed state of the environment. This is called a \emph{policy} and need not be deterministic: one may perfectly well have a policy that states that, when faced with an environment in a certain state, the agent should randomly pick an action from a given probability distribution defined over the set of admissible actions. Ultimately, the "answer" for a Reinforcement Learning problem is a certain policy i.e. a function that connects each state in the environment, either to an action to be taken (if the policy is deterministic) or to a probability distribution from which the action is to be drawn (if the policy is stochastic).

By "present value", we mean the exact same thing that is meant in finance by the homonymous expression. Namely, that imminent rewards are perceived as more valuable than similar rewards in a distant future, so we apply a \emph{discount factor} for every period we have to wait in order to perceive a reward. As a result, the values of rewards decrease exponentially with the time we have to wait to perceive them.

If we were to combine all these ideas into  a single mathematical expression, we could state that the goal of RL is to solve the following problem, given a set of problem-specific initial conditions and constraints:

$$
\pi^*=\arg \max_{\pi} \mathbb{E}_\pi \left[ \sum_{t=0}^{+\infty} \gamma^t r(s_t,a_t) \right ] \label{1}
$$

Here, $\gamma$ is the discount factor and  $r(s_t,a_t)$ is the reward obtained at time $t$ as a result of taking the action ($a_t$)  prescribed by policy $\pi$ given the state ($s_t$) the agent finds itself at that time. The reader may recognize the term within the square brackets as the present value of a cash flow of rewards. But since the policy is arguably stochastic, there is uncertainty on the exact course of action to be followed and therefore on the rewards to be perceived. Thus we cannot maximize the present value of expected rewards, but only its expected value given the policy the agent undertakes. Our goal is to find the policy that maximizes such expected value.

In particular, if $\gamma \to 0$, our policy consists on choosing what is the immediate best solution, with no regards for what might happen in future time steps. It's a greedy policy. Conversely, with $\gamma\to1$, no discounting is in place, and similar rewards are perceived as having equal values regardless on how much the agent has to wait in order to perceive them.

Furthermore, note that no mention is made as to how the environment moves from one state to another, or the probabilities with which states succeed each other. All that is necessary is to observe the rewards that result from following the prescribed policy. In this sense, RL is\emph{model free}. This is an advantage of RL for real life problems, where we are often ignorant on how the real world's inner workings, and are only conscious of our actions and their consequences. This is, in a way, quite poetic. There is no blueprint to life. We learn by doing. By acting, and facing the consequences of our actions. That's how RL works, and that's a feature that makes it very applicable to so many real-life problems.

In addition, states need not be perfectly observed. Thus, in the first paragraphs of this section, we were careful in our choice of words, when we said  that "agent observes certain features of the environment and acts on what he observes". The agent need not observe \emph{all} features of the environment. It need not even observe these features as they truly are. But whatever it does observe, that's what it acts upon. Our agent will try to find the best actions it can, given the information it has, however partial or noisy they may be. The fact that information may not be neither complete nor perfect is very realistic, and accounts for another advantage of RL over other ML approaches such as Supervised or Unsupervised Learning, with are much more silent about these issues.

\subsection{Overview of heuristics for solving the Reinforcement Learning Problem}

So how does one go about solving Equation $\ref{1}$?

There are essentially two ways to do this: one can either follow a \emph{value-based approach} or a \emph{policy-based approach}.

In a \emph{value-based approach}, one tries to determine the value of taking each possible action on each possible state. By value we mean the expected value in Equation $\ref{1}$ i.e. the thing which we want to maximize. If an agent knows the value of taking each possible action on each possible state, it can easily figure out the best action to take in whichever state it happens to be in. All it has to do is pick whichever action moves it to the next state of highest value.

One challenge to this approach arises when the number of states is very large, and in particular, when it is infinite. In this scenarios, some states may only seldom be visited, if they are visited at all. One way to deal with this is by encoding each state by a finite vector of essential features $\phi$. It's as if, rather than having a list of infinite possible states, we had a small set of features that matter for decision making. Mathematically, this means we move from having an infinite number of possible states, $\{s_0, s_1,s_2...\}$, to having a function $s(\phi)$, which enables us to estimate which state we are in based on a small set of features $\phi$. Since it is impossible to visit an infinite number of states, one must learn how to infer the rewards of each state based on the values of $\phi$ observed for the set of states that actually were visited. This task is often accomplished with Deep Learning techniques.

In a \emph{policy-based approach}, one tries to determine the optimal policy directly, without resorting to the intermediate step of calculating the values of taking each action on each possible state. This is done by parametrizing the policy as a function $\pi_\theta(a,s)$ that assigns a probability of taking an action $a$ when on a state $s$, and then seeking to find the optimal values of the parameter vector $\theta$. 

\subsection{The Soft Actor-Critic (SAC) algorithm}

Each of these approaches has its strengths and weaknesses. One approach to combine both approaches and thus leverage their strengths is the \emph{actor-critic} algorithm. The "actor" acts following a certain policy. The "critic" observes the rewards it perceives and uses this to calculate the values of each action at each state. These values are then used by the actor to estimate the optimal policy. It then enacts this policy, providing more information for the critic to update its values estimated. And thus, both actor and critic improve their respective performances, until the actor's estimate of the optimal policy converges.

In practice, the actor and the critic are two neural networks. The critic tries to find the function that relates $\phi$ to the rewards perceived at a given state, while the actor tries to find the parameters $\theta$ that lead to the optimal performance $\pi_\theta$. Both neural networks interact and evolve together. This approach of combining neural networks is no novelty to AI literature, and has been successfully implemented in applications such as Generative Adversarial Neural Networks or Variational Autoencoders.

The actor-critic algorithm can be improved by fostering exploration. In all RL algorithms, there is an inherent concern over how to balance exploration and exploitation. The latter (exploitation) consists of taking those actions that are already known to perform the best, while the former (exploration) consists of attempting actions which may seem senseless on the hopes that they may help us discover a better policy than the one we currently have. No learning is possible without some degree of exploration. Yet, since exploration leads to failure much more often than it leads to success, it's easy to overlook it. This is especially striking in continuous spaces of action, where the probability of exploring any given specific action tends to zero, as does the probability of that specific action being the jackpot. So how can we encourage an agent to explore new courses of action on a continuous-space of possible actions?

The \emph{soft-actor-critic} (SAC) algorithm does that by adding an additional term to the traditional RL objective function. This function is (proportional to) the entropy of the policy. Hence, it praises policies which acknowledge doubt on to what is the best course of action to pursue, and punishes policies that are overly confident that only one specific action can be taken in any  given case. Under this framework, Equation $\ref{1}$ becomes:

$$
\pi^*=\arg \max_{\theta} \mathbb{E}_{\pi(\theta)} \left[ \sum_{t=0}^{+\infty} \gamma^t r(s_t(\phi),a_t) + \alpha \mathcal{H_\pi}\right ] \label{2}
$$

where $\mathcal{H_\pi}$ is the entropy of policy $\pi$, now parametrized in terms of $\theta$, and $\alpha$ is a measure of how much one wishes to penalize a policy for failing to explore. The critic will try to arrive at the value of $\phi$ best estimates  $Q_\pi(s,a)=\sum_{t=0}^{+\infty}r(s_t(\phi),a_t)$, while the actor will attempt to find the value of $\theta$ that produces the optimal policy. Note that $Q_\pi(s,a)$, the function estimated by the critic, is dependent on $\pi$, the policy provided by the actor. Conversely,  $\pi^*$, the policy that the actor believes to be optimal, is dependent on $Q_\pi(s,a)$, the function estimated by the critic.

\subsection{Imitation Learning}

There is yet another way to solve the Reinforcement Learning that evades solving equation $\ref{1}$ altogether. Rather than letting a computer figure out the best policy by acting and suffering the consequences of its actions, why not teach the computer simply to copy the behavior of someone who already knows what to do?

The task therefore becomes to find out what would an "expert" do in \emph{every} situation given that we have a dataset of what he said should be done in \emph{some} situations. The "expert" behavior thus becomes a target, and our goal is to find a predictor for it. Our reinforcement learning problem is thus reduced to a classical supervised learning problem [@rashidinejad2021].

Granted, the performance of the trained algorithm will be upper bounded by the performance of the expert. Such caveat notwithstanding, this approach is very convenient when an "expert" is easily available, and when quantifying the rewards of each action is difficult [@smartlab2021]. In other words, imitation learning is useful when it is easier for an expert to show what must be done than to actually figure out the rewards for each possible action.

Such is the case, for example, on autonomous cars [@pomerleau-2021]. One can easily find a driver and observe that he signals a left turn before actually turning left. Quantifying the reward obtained by such signalling, however, is much harder.

Another advantage of imitation learning is that it draws upon algorithms from classical supervised learning, which tend to be much more simpler and faster to run than those of RL.

A downside of using classical supervised learning algorithms, however, is that these algorithms often suppose that training samples are independent. This is seldom the case. In the self-driving car example, what the driver sees in one moment cannot be said to be independent from what he will see in the following moment. Quite on the contrary, the latter is a direct consequence of the actions taken by him on the former. By treating observations as independent, errors committed in one circumstance add up, leading the algorithm to learn a behavior that may ultimately be very different than what the expert would actually do [@rashidinejad2021].

Having reviewed the field of RL from a general point of view, we now present the simulator we will be using for this paper. Subsequently, we will delve into how extant literature has employed RL to tackle the challenges of power grid management and, in particular, to address the issue of demand response.

\subsection{The City Learn power grid simulator}

City Learn [@Vazquez-Canteli2019] is a simulation environment for energy demand response. In its simplest form, it considers a generator (that supplies energy) and a building (that consumes energy). Demand for energy varies according to time zone and time of day. The building can stock energy during periods of low-demand to use during periods of high-demand, but there's a limit to how much energy the building can stock. The building seeks to minimize a cost function which considers a series of factors: amount spent on energy, risk of shortage, risk of rampage, and others.

In a not-so-simple scenario, there are multiple generators and multiple buildings. An agent decides how much energy should be used by or stocked at each building, so as to minimize the cost function for the *network* (not any individual building). There are various different types of energy that can be considered, such as thermal energy, batteries, and air-to-water heat pumps. While they can all be stocked, they do have different prices.

To be more specific, City Learn simulates an environment where a set of $N$ buildings are, at any given time $t$, in a state that is completely defined by 28 variables. These variables are of the following types:

\begin{itemize}
\item Temporal variables as ‚Äúmonth‚Äù, ‚Äúday‚Äù, or ‚Äúhour‚Äù
\item Temperature variables
\item Humidity variables
\item Diffuse solar radiation variables
\item Storage variables(How much energy is stored)
\end{itemize}

We find it important to highlight that all but the last type of variable are related to the demand of energy. Indeed, the demand for energy varies through time depending on temperature , humidity or the speed with which energy dissipates. The last kind of variable, however, refers to storage. And storage is ultimately a "managerial" decision: it's how the agent chooses to manage the network. Such choice has implications that deserve our attention: If a building stocks energy today, it is betting that energy today will be cheaper than in the future. Moreover, if it chooses to use energy it has previously stocked, it is limited by the choice of how much it decided to stock in the past. Hence, the storage variables insert a path dependency into our problem.

As for the actions, there are only 2 actions possible for the agent to take

\begin{itemize}
\item Increasing or decreasing the energy in cooling storage
\item Increasing or decreasing the energy in domestic hot water storage
\end{itemize}

Since each building has its own storage, such 2 actions are, in reality, 2 actions \emph{per building}. In other words, the agent must decide how much energy will put in storage (or taken out of storage) for each building.

Such agent may be a single agent, managing the entire network, or a set of agents, one responsible for each building, which act independently but can communicate with each other.

Having presented both the field of RL and the simulator we will employ, we now proceed to discuss how RL scholars have previously addressed the issue of power grid management. We shall be particularly concerned with a particular problem in power grid management (namely, demand response) for which we will give a precise definition in the upcoming section. Finally, we will show how this present paper articulates itself with extant literature, and how it relates to the ongoing debate on how can RL be used to improve current practices of power grid management. 

\subsection{Reinforcement Learning for demand response}

The task of managing a power grid conforms well to the basic tenants of RL. For each action one takes on a power grid, one has a corresponding consequence to either enjoy or suffer. The grid can have a myriad of different states, the transition probabilities of which are unknown. In short, one must teach an agent which actions to take given only the rewards of such actions. That's the quintessential example of RL [@Lambert2020]. Indeed, there is a vast literature on employing RL to managing power grids (e.g. [@Ademoye2012; @Hadidi2013; @Glavic2017; @Mocanu2019]).

[@Glavic2017] performed a literature review where he analyzed extant challenges and concerns about the employment of RL to power grid management concluding the overall sentiment in the scientific community is that RL is a very promising approach to deal with the task at hand.

Indeed, RL has been employed to multiple problems, all of which fall under the general category of power grid management. Thus, some authors focus on optimizing the generation of electricity ([@Wang2016]), others focus on its distribution ([@Gemine2017]), while others focus on managing the consumption of energy itself ([@Xu2020; @Mocanu2019; @SI2021]). Likewise, some authors focus on controlling the power grid ([@Yousefian2016; @Yousefian2016]), others in stabilizing it ([@Hadidi2013]), while others attempt to modulate demand by setting energy prices dynamically ([@Tsui2012]).

This latter stream of research, in particular, has been receiving a significant share of scholarly attention, and even has a name of its own: demand response. Demand response consists of managing the power grid by modulating the demand for energy. This will typically be done by some sort of price incentive, lowering energy prices when one wishes to increase demand, and raising energy prices when one wishes to decrease it [@Christensen2020; @Kurte2020]. Our paper falls into this category. 

We consider the problem of a small power grid to which buildings of different kinds are connected. The price of electricity varies with time and, in particular, it increases as a response to an unexpected ramping in demand. Buildings use electricity to either heat up or cool down, thereby maintaining a pleasant temperature. The need to heat up or cool down is determined by the outside temperature which varies throughout the year in a way that is specific to the climate zone where the power grid is located. Buildings have the option to store either hot or cold water, that can be used to either heat up or cool down the building at some later time. Hence, if a building is anticipating hot weather on the upcoming days and the price of electricity is cheap at present, it may wish to store cold water, so it can cool itself down when the temperature rises and energy prices are no longer cheap. In making such decisions, however, we assume buildings act independently to each other, with no information sharing between them. All data is simulated, and we use the City Learn package to this end.

Unlike supervised and unsupervised learning, RL often makes use of simulated data (with some exceptions e.g. [@Xu2021]). [@Kang2019] notes that one reason for the ubiquity of simulated data on RL research is because RL requires copious amounts of data, to the extent where getting such data might be very costly, if at all feasible.  [@DeepMind2021] makes an interesting remark on this point. Namely,  [@DeepMind2021] argue that, in some real-world application domains, data may be generated slowly. This is, indeed, the case with our paper. In order to learn how to efficiently manage  energy throughout yearly cycles of succeeding seasons, one would need many years of data. By using simulations, such data can be obtained with no need to wait. 

There is, of course, a trade-off here. While simulated data is much faster and cheaper to obtain in large amounts, it may fail to accurately replicate what real-data would look like. In plain terms, one can always cast doubt on whether a simulator is indeed realistic ([@Dulac2019]). This has led some researchers to endeavor to combine real-data and simulated data ([@Kang2021; @Hanna2019; @Mocanu2019]). 

In spite of such promising developments in the field, there has been a remarkable development in simulators targeted at RL research (e.g. [@DeepMind2021; @OpenAIGym]). At present, most research on RL relies on simulated data. Our paper follows this tradition, and uses the City Learn simulator [@Vazquez-Canteli2019].

The City Learn simulator simulates a small power grid network. As concerns its size, our grid is an intermediate between the one modelled by [@Kuznetsova2013] (which consists of a consumer, a generator and a battery for storage), and the one modelled by [@Yousefian2016] (where dispersion over a large area is a feature that must be considered). In our power grid, there are multiple buildings, but they are sufficiently close together for delays between generation and consumption to be neglected. 

In addition, we have multiple buildings and pay no attention to how is energy distributed inside a building. In this respect, our paper is opposite to that of [@Tsui2012], where one considers a single home but worries about the multiple appliances it contains. 

It is important to highlight, however, that 9 different types of buildings are considered. This should arguably enhance the adherence of our simulated power grid to reality, since in the real-world, there exists buildings with different demands for energy. A commercial building has high demand for electricity during work hours, but a smaller demand during the weekend. The opposite is true for residential buildings.  

Likewise, we consider 4 different clime zones, which means there are 4 distinct ways to simulate how temperature (and hence energy demand) evolves throughout the year. 

In our simulator, buildings can use energy to store either warm or cold water, that will subsequently be used for cooling or heating. Scholars such as [@Tsui2012; and @Xu2020] allow for many more uses of electricity within a building, but their interest lies on a single household. Conversely, scholars such as [@Ademoye2012; and @Mocanu2019] deal with the energy demand as a black box, with no distinction whatsoever as regards its use. Our paper strikes a balance between both approaches: while not delving into all the possible uses of electricity within a building, by allowing two possible uses, it acknowledges that this utility may be used in different ways.

In addition, it's worthy to note that buildings can only store or use energy. They cannot generate energy. In this regard, our approach differs from [@Wang2016], where buildings are equipped with photovoltaic generating systems. This should arguably increase the need for energy storage. While data from the International Energy Agency ([@IEA2020]) suggests global photovoltaic capacity has been doubling every 2.4 years, the same report shows that such capacity is capable of meeting only 3% of the global demand for energy. Hence, at present, it is more realistic to suppose buildings do not have the ability to generate their own energy than to suppose otherwise.

When all such buildings are brought together and integrated into a power grid, one must make the decision as to how such grid is to be managed. One possibility, for example, is to have a single agent decide how each building should act, so as to minimize the cost of managing the entire grid. Such an agent would need to have information on each building's energy needs and storage levels. In its own turn, the agent would combine all this information into a single objective function that considered the costs of all buildings, thus achieving what Economic literature calls the social optimum. [@Yousefian2016] is an example of such an implementation. 

One practical problem of the single agent approach is that granting it information on the energy consumption of each individual building may be perceived as a violation of privacy rights. Moreover, granting control over the power grid to a single agent raises concerns over malpractices such as observed during the California Energy Crisis [@California-Energy-Crisis]. Finally, minimizing the collective cost of energy doesn't necessarily guarantee that each individual building will have its own cost minimized, which may also pose an issue in more individualistic societies. 

Another possibility as to how to manage a power grid is to do precisely the opposite: have each building decide on its own how much energy to use or to store. In plain terms, rather than having a single agent decide what everyone should do, each building shall decide for itself. In this multi-agent approach, each building optimizes its own objective function. In principle, nothing precludes buildings from sharing information among themselves, so it is still possible to incorporate information from multiple buildings within each objective function. Still, since each building optimizes its own objective function (possibly conditioned on its expected behavior of the remaining buildings), there is no guarantee that a social optimum will be achieved. Nonetheless, concerns over privacy, excessive market power and the limitation of the individual's freedom to chose have been greatly reduced. [@Li2012; @Dhamankar2020; @Christensen2020; @Xu2020; and @Ademoye2012] are all examples of this approach.

In our paper, we use a multi-agent approach with no information sharing. This means each building maximizes its own objective function, with no regards whatsoever for other buildings. We chose a multi-agent approach because, in our understanding, it is more realistic, as it evades the caveats cited previously. In addition, by doing so, we align ourselves with the majority of previous research. A third, but equally important reason, results from a meeting we had with the developer of the City Learn simulator, in which he suggested we used the multi-agent approach, since its code is more stable, and has been more tested, than the single agent.

In reality, a hybrid model is often observed, where a large network is broken into regions and, within each region, the generation, transmission and distribution of energy are managed by a single-agent fashion, whereas the buildings are granted autonomy to decide for themselves how much to consume or store. In other words, within a certain region, the decisions pertaining generation, transmission and distribution are made by centralized agents, whereas the actual consumption of energy is made by decentralized agents. The Synchronous grid of Continental Europe [@SGCE], for example, is managed by the European Network of Transmission System Operators for Electricity (ENTSO-E), an organization that is, in turn, comprised by 42 other organizations (called Transmission System Operators, or TSO) that manage the generation, transmission and distribution of electricity within specific regions of the grid. Since the entry costs of generation, transmission and distribution are formidably high, these activities are natural monopolies, so concerns over market power of TSOs cannot be solved by competition. Thus, a single agent approach lends itself naturally to this scenario. The same cannot be said, however, for the demand side of the grid. Hence, as far as buildings are concerned, they are autonomous to make their own decisions regarding energy consumption. Here, what we have is a multiple-agent approach. We have been unable to find any papers that employ such a hybrid model of power grid management. While we acknowledge that such vacuity may be a result of the increased complexity of a hybrid model, we should also point out that this is (to the best of our knowledge) an open field for future research.


\section{Experimental results}
\subsection{Data}

\subsubsection{Simulator}
- Describe the environment(especially about what kinds of data are generated from the environment) 

CityLearn is an OpenAI Gym environment for the easy implementation of RL agents in a multi-agent demand response setting to reshape the aggregated curve of electrical demand by controlling the energy storage of a diverse set of buildings \cite{CityLearn}. Its main objective is to facilitate and standardize the evaluation of RL agents such that it enables benchmarking of different algorithms. CityLearn includes energy models of air-to-water heat pumps, electric heaters, and the pre-computed energy loads of the buildings, which include space cooling, dehumidification, appliances, domestic hot water (DHW), and solar generation. We try to implement a new reinforcement algorithm (RL) into CityLearn to control the storage of DHW and chilled water. The RL agents send their control actions hourly and receive a set of states and rewards in return. Indoor temperatures in the buildings do not change, as the environment automatically constraints the actions of the controllers and ensures that the energy supply devices are large enough to guarantee that the energy demand of the buildings is always supplied. The RL agents can decide how much cooling or heating energy store or release at any given time. A backup controller integrated in CityLearn guarantees that the energy supply devices prioritize satisfying the energy demand of the building before storing any additional energy. The simulator allows us to configure the building properties and their number to be controlled by the agent(s). In addition, it also provides four climate zone setting to evaluate if the agent can works generally in several environments. As Figure \ref{fig:ave_tmp_by_cz} shows, the climate properties, such as temperature inside/outside the buildings differ by the climate zone.

\begin{figure}[h!]
  \includegraphics[scale = 0.3]{figures/temp_by_climate_zone.png}
  \caption{Average indoor temperature setting by climate zone}
  \label{fig:ave_tmp_by_cz}
\end{figure}

\subsubsection{State, reward, and action for reinforcement learning agent}

\newline
- Describe about different parameters for the environment(for example explain about region
parameter)
- Describe who is the agent. Describe what are the possible actions. Describe how is reward calculated. 
\newline
- Detail number of simulations (and parameters) actually employed. Brief descriptive statistics

City Learn simulates an environment where a set of N buildings are, at any given time t, in a state that is completely defined by 28 variables. These variables are of the following types:
\begin{itemize}
\item Temporal variables as ‚Äúmonth‚Äù, ‚Äúday‚Äù, or ‚Äúhour‚Äù ‚Ä¢ Temperature variables
\item Humidity variables
\item Diffuse solar radiation variables
\item Storage variables(How much energy is stored)
\end{itemize}
\newline

We find it important to highlight that all but the last type of variables are related to the demand for energy. Indeed, the demand for energy varies through time, with temperature, humidity, or with the speed with which energy dissipates. The last kind of variable, however, refers to storage. And storage is ultimately a ‚Äúmanagerial‚Äù decision: it‚Äôs how the agent chooses to manage the network. Such choice has implications that deserve our attention: If a building stocks energy today, it is betting that energy today will be cheaper than in the future. Moreover, if it chooses to use the energy it has previously stocked, it is limited by the choice of how much it decided to stock in the past. Hence, the storage variables insert a path dependency on our problem.
As for the actions, there are only 2 actions possible for the agent to take - Increasing or decreasing the energy in cooling storage - Increasing or decreasing the energy in domestic hot water storage since each building has its own storage, such 2 actions are, in reality, 2 actions per building. In other words, the agent must decide how much energy will de put in storage (or taken out of storage) for each building. Such a central agent may be a single agent, managing the entire network, or a set of agents, one responsible for each building, which acts independently but can communicate with each other. We have considered a set of one agent per building acting independently. For simplicity, however, we often refer to these agents in a singular form.


\subsection{Methods}

Description on what part of pipeline improved compared to original paper
\newline
\indent - PCA into Auto-encoder
\newline
\indent - Gradient boosting decision tree into Recurrent Neural Network
\newline
\indent - Improved reward functions through incorporating whether demand curve is flattened or not.
\newline
Evaluation of models
\newline
\indent ‚Äì consider different baselines, which will from original paper that has 4 different agents that are trained with different reward function.
\newline
\indent \indent - $r_{i}^{1} = min(0,e_{i})$
\newline
\indent \indent - $r_{i}^{2} = sign(e_{i}) \times min(0,e_{i})^{2}$
\newline
\indent \indent - $r_{i}^{1} = min(0,e_{i})^{3}$
\newline
\indent \indent - $r_{i}^{1} = -sign(e_{i})\times e_{i}^{2} \times min(0,\Sigma e_{i})$
\newline
\indent - compare through evaluating these 5 objectives below. 
\newline
\indent \indent Peak demand (for the entire simulated period)
\newline
\indent \indent - Average daily peak demand (daily peak demand of the district averaged over a year)
\newline
\indent \indent - Ramping
\newline
\indent \indent - 1 - Load factor (which will tend to 0 as the load factor approaches 1)
\newline
\indent \indent - Net electricity consumption

\subsection{Evaluation of Results}

- Compare the results of all models. 
\newline
- First we are going to separately change one by one. For example check whether auto-correlation will improve separately. 
\newline
- We are going examine how this change impacts one of the objective. We are going to do this for three methods we have mentioned
\newline
- For final result we are going to combine three method we have discussed to see how those three changed have impact together.
\newline
- We also expect some trade-offs from some of the method we add and we are going to probably talk about in this section. Discuss about how to handle the trade-offs if there is any. 

\section{Conclusion/Discussion}

\subsection{MAML}

The limitations of the original paper and our method are in generalization and the policy's long training period. We have shown that policy does not do well on generalization over other regions, especially compared to the trained regions in the cost. Due to a lack of generalization, we might need to train the policy for each region separately. However, it takes at least ten years to train an adequate policy. To address this problem, we plan to incorporate Model-Agnostic Meta-Learning(MAML) algorithm, which will deal with over-fitting problems and decrease the policy's training period.\cite{finn2017modelagnostic} There are numerous interpretation of MAML. One of them is that the algorithm is designed to make the policy to learn shared components between different regions. It learns shared components by averaging different gradient descents updates from each region. In more detail, it will sample k trajectories from each n regions. It will update its parameters for n policies, copies of the original policy, using gradient descent. Using those n updated policies, it will sample another h trajectories. It will update the original policy by averaging those different gradient descents updates from n updated policy. After learning shared components, we would need to train the model further in each region to learn its unique components compared to other regions. Since the policy only has to learn the unique components after using MAML, the required training period will decrease. 

We did face some obstacles from implementing MAML into our environment. Most of the packages for MAML are for a single agent. However, our citylearn environment has multiple agents. Furthermore, we are interested in optimizing the policy in a competitive environment, so we also need to optimize other remaining policies to establish this environment. We thought our problem was similar to Gibbs sampling problem, a sampling problem for distributions that has a dependent relationship, as our optimizing agent also depends on other agents' policy. In this case, we cannot directly optimize or sample from a single agent or single parameter. Inspired by Gibbs sampling algorithm, we would iteratively update each policy, so each iteration the policy would be, in theory, optimized in a more competitive environment than previous iterations. The pseudocode for our algorithm is written in \textbf{Algorithm\ref{algo:multi-maml}}. We are currently working on this problem. 

\begin{algorithm}
    \caption{Multi-agent MAML}
    \label{algo:multi-maml}
  \begin{algorithmic}[1]
    \REQUIRE $P(Z):$ distribution of zones
    \REQUIRE $\theta_{i}:$ policy for each building in the environment
    \REQUIRE $\alpha$, $\beta:$ step size parameter for update
    \WHILE{not done}
        \FOR{\textbf{all} $\theta_{i}$}
        \State sample batch of zones $Z_{j} \sim P(Z)$
            \FOR{\textbf{all} $Z_{j}$}
                \State Sample K trajectories $D = \{(x_{1}, a_{1}, ...x_{H})\}$ using $f_{\theta_{i}}$ in the environment with zone $Z_{j}$
                \State Evaluate $\nabla_{\theta_{i}}L_{Z_{j}}(f_{\theta_{i}})$ using $D$ and $L_{Z_{j}}$
                \State compute adapted parameters using gradient descent $\theta^{'}_{i,j} = \theta - \alpha \nabla_{\theta_{i}}L_{Z_{j}}(f_{\theta_{i}})$
                \State Sample trajectories $D^{'}_{j} = \{(x_{1}, a_{1}, ...x_{H})\}$ using $f_{\theta^{'}_{i,j}}$ in zone $Z_{j}$
            \ENDFOR
            \State Update $\theta_{i} = \theta_{i} -\beta \nabla_{\theta_{i}}\sum_{Z_{j} \sim P(Z)} L_{Z_{j}}(f_{\theta_{i,j}})$ using each $D^{'}_{j}$ and $L_{Z_{j}}$
        \ENDFOR
    \ENDWHILE
  \end{algorithmic}
\end{algorithm}

We will mention about the limitation of our model and talk about what kind of different methods we could possibly consider in the future. 
\newline
We will also mention about limitation in environment.It would be great if we could find more environment to make our model more generable. 

\section{Figures}
\begin{figure}[h]
\centering
\includegraphics[scale=0.4]{figures/pipeline}
\caption{Going to highlight part of pipelines that we are going to change to improve}
\end{figure}
\begin{figure}[h]
\centering
\includegraphics[scale=0.4]{figures/replication_graph}
\caption{We are going to have other lines that we are going to try to improve. Currently those 4 lines will be a baseline for us}
\end{figure}





\newpage

\bibliographystyle{plain}
\bibliography{References.bib}

\end{document}