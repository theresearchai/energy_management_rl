Reinforcement Learning (RL) is one of the branches of Machine Learning (ML), alongside with Supervised and Unsupervised Learning. In RL, there is an agent and an environment. The agent observes certain features of the environment and acts on what he observes, taking some action. As a result of the action he takes, he receives a certain reward and changes the environment in some way.  He then observes certain features of this new environment, and takes a new action. As a result, he gets a new reward and the environment is changed again. Thus the agent takes a succession of _actions_ based on what it perceives as being the _state_ of the environment, and receives a _reward_ for each action. The goal of RL is to teach the agent how to act in order to maximize the present value of the expected the reward it earns. Two terms are important in this statement: "teach how to act" and "present value."

By "teach the agent how to act", we mean deciding which action to take for each observed state of the environment. This is called a _policy_ and need not be deterministic: one may perfectly well have a policy that states that, when faced with an environment in a certain state, the agent should randomly pick an action from a given probability distribution defined over the set of admissible actions. Ultimately, the "answer" for a Reinforcement Learning problem is a certain policy i.e. a function that connects each state in the environment, either to an action to be taken (if the policy is deterministic) or to a probability distribution from which the action is to be drawn (if the policy is stochastic).

By "present value", we mean the exact same thing that is meant in finance by the homonymous expression. Namely, that imminent rewards are perceived as more valuable than similar rewards in a distant future, so we apply a _discount factor_ for every period we have to wait in order to perceive a reward. As a result, the values of rewards decrease exponentially with the time we have to wait to perceive them.

If we were to combine all these ideas into  a single mathematical expression, we could state that the goal of RL is to solve the following problem, given a set of problem-specific initial conditions and constraints:
$$
\pi^*=\arg \max_{\pi} \mathbb{E}_\pi \left[ \sum_{t=0}^{+\infty} \gamma^t r(s_t,a_t) \right ] \label{1}
$$
Here, $\gamma$ is the discount factor and  $r(s_t,a_t)$ is the reward obtained at time $t$ as a result of taking the action ($a_t$)  prescribed by policy $\pi$ given the state ($s_t$) the agent finds itself at that time. The reader may recognize the term within the square brackets as the present value of a cash flow of rewards. But since the policy is arguably stochastic, there is uncertainty on the exact course of action to be followed and therefore on the rewards to be perceived. Thus we cannot maximize the present value of expected rewards, but only its expected value given the policy the agent undertakes. Our goal is to find the policy that maximizes such expected value.

In particular, if $\gamma \to 0$, our policy consists on choosing what is the immediate best solution, with no regards for what might happen in future time steps. It's a greedy policy. Conversely, with $\gamma\to1$, no discounting is in place, and similar rewards are perceived as having equal values regardless on how much the agent has to wait in order to perceive them.

Furthermore, note that no mention is made as to how the environment moves from one state to another, or the probabilities with which states succeed each other. All that is necessary is to observe the rewards that result from following the prescribed policy. In this sense, RL is _model free_. This is an advantage of RL for real life problems, where we are often ignorant on how the real world's inner workings, and are only conscious of our actions and their consequences. This is, in a way, quite poetic. There is no blueprint to life. We learn by doing. By acting, and facing the consequences of our actions. That's how RL works, and that's a feature that makes it very applicable to so many real-life problems.

In addition, states need not be perfectly observed. Thus, in the first paragraphs of this section, we were careful in our choice of words, when we said  that "agent observes certain features of the environment and acts on what he observes". The agent need not observe _all_ features of the environment. It need not even observe these features as they truly are. But whatever it does observe, that's what it acts upon. Our agent will try to find the best actions it can, given the information it has, however partial or noisy they may be. The fact that information may not be neither complete nor perfect is very realistic, and accounts for another advantage of RL over other ML approaches such as Supervised or Unsupervised Learning, with are much more silent about these issues.

So how does one go about solving Equation $\ref{1}$?

There are essentially two ways to do this: one can either follow a _value-based approach_ or a _policy-based approach_.

In a _value-based approach_, one tries to determine the value of taking each possible action on each possible state. By value we mean the expected value in Equation $\ref{1}$ i.e. the thing which we want to maximize. If an agent knows the value of taking each possible action on each possible state, it can easily figure out the best action to take in whichever state it happens to be in. All it has to do is pick whichever action moves it to the next state of highest value.

One challenge to this approach arises when the number of states is very large, and in particular, when it is infinite. In this scenarios, some states may only seldom be visited, if they are visited at all. One way to deal with this is by encoding each state by a finite vector of essential features $\phi$. It's as if, rather than having a list of infinite possible states, we had a small set of features that matter for decision making. Mathematically, this means we move from having an infinite number of possible states, $\{s_0, s_1,s_2...\}$, to having a function $s(\phi)$, which enables us to estimate which state we are in based on a small set of features $\phi$. Since it is impossible to visit an infinite number of states, one must learn how to infer the rewards of each state based on the values of $\phi$ observed for the set of states that actually were visited. This task is often accomplished with Deep Learning techniques.

In a _policy-based approach_, one tries to determine the optimal policy directly, without resorting to the intermediate step of calculating the values of taking each action on each possible state. This is done by parametrizing the policy as a function $\pi_\theta(a,s)$ that assigns a probability of taking an action $a$ when on a state $s$, and then seeking to find the optimal values of the parameter vector $\theta$. 

Each of these approaches has its strengths and weaknesses. One approach to combine both approaches and thus leverage their strengths is the _actor-critic_ algorithm. The "actor" acts following a certain policy. The "critic" observes the rewards it perceives and uses this to calculate the values of each action at each state. These values are then used by the actor to estimate the optimal policy. It then enacts this policy, providing more information for the critic to update its values estimated. And thus, both actor and critic improve their respective performances, until the actor's estimate of the optimal policy converges.

In practice, the actor and the critic are two neural networks. The critic tries to find the function that relates $\phi$ to the rewards perceived at a given state, while the actor tries to find the parameters $\theta$ that lead to the optimal performance $\pi_\theta$. Both neural networks interact and evolve together. This approach of combining neural networks is no novelty to AI literature, and has been successfully implemented in applications such as Generative Adversarial Neural Networks or Variational Autoencoders.

The actor-critic algorithm can be improved by fostering exploration. In all RL algorithms, there is an inherent concern over how to balance exploration and exploitation. The latter (exploitation) consists of taking those actions that are already known to perform the best, while the former (exploration) consists of attempting actions which may seem senseless on the hopes that they may help us discover a better policy than the one we currently have. No learning is possible without some degree of exploration. Yet, since exploration leads to failure much more often than it leads to success, it's easy to overlook it. This is especially striking in continuous spaces of action, where the probability of exploring any given specific action tends to zero, as does the probability of that specific action being the jackpot. So how can we encourage an agent to explore new courses of action on a continuous-space of possible actions?

The soft-actor-critic (SAC) algorithm does that by adding an additional term to the traditional RL objective function. This function is (proportional to) the entropy of the policy. Hence, it praises policies which acknowledge doubt on to what is the best course of action to pursue, and punishes policies that are overly confident that only one specific action can be taken in any  given case. Under this framework, Equation $\ref{1}$ becomes:
$$
\pi^*=\arg \max_{\theta} \mathbb{E}_{\pi(\theta)} \left[ \sum_{t=0}^{+\infty} \gamma^t r(s_t(\phi),a_t) + \alpha \mathcal{H_\pi}\right ] \label{2}
$$
where $\mathcal{H_\pi}$ is the entropy of policy $\pi$, now parametrized in terms of $\theta$, and $\alpha$ is a measure of how much one wishes to penalize a policy for failing to explore. The critic will try to arrive at the value of $\phi$ best estimates  $Q_\pi(s,a)=\sum_{t=0}^{+\infty}r(s_t(\phi),a_t)$, while the actor will attempt to find the value of $\theta$ that produces the optimal policy. Note that $Q_\pi(s,a)$, the function estimated by the critic, is dependent on $\pi$, the policy provided by the actor. Conversely,  $\pi^*$, the policy that the actor believes to be optimal, is dependent on $Q_\pi(s,a)$, the function estimated by the critic.



