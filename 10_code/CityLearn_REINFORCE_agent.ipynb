{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CityLearn_REINFORCE_agent.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_OWi-NBKDBU"
      },
      "source": [
        "# CityLearn MAML agent\n",
        "Name: Shota Takeshima\n",
        "* before implementing MAML, I understand how multiple baseline agent works and remove unnecessary parts, such as information sharing and PCA state deimentionality reduction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xXcoRQaXvCz"
      },
      "source": [
        "## Installation: CityLearn\n",
        "\n",
        "* Temporally, I put CityLearn codes in my private git repository to test it. \n",
        "* After some progress is confirmed, I'll push the modified code to our team repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edOW6fSdX_g9",
        "outputId": "35cf2238-e46c-42e0-e6ad-35dd5573de4e"
      },
      "source": [
        "!rm -rf ./CityLearn_garage/\n",
        "!git clone https://github.com/shttksm/CityLearn_garage.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'CityLearn_garage'...\n",
            "remote: Enumerating objects: 116, done.\u001b[K\n",
            "remote: Counting objects: 100% (116/116), done.\u001b[K\n",
            "remote: Compressing objects: 100% (98/98), done.\u001b[K\n",
            "remote: Total 116 (delta 23), reused 107 (delta 17), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (116/116), 11.92 MiB | 12.85 MiB/s, done.\n",
            "Resolving deltas: 100% (23/23), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QQZwH9x2Y0w"
      },
      "source": [
        "## Confirm the baseline multiple agent, RL_Agents_Coord\n",
        "* RL_Agents_Coord is in `CityLearn_garage/agent.py`. Each building has itw own SAC agent and this class manages such SAC agents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpITDUjVavZE",
        "outputId": "9e896e47-a3d4-42fa-8034-2dc34e83e1fb"
      },
      "source": [
        "!grep RL_Agents_Coord CityLearn_garage/agent.py"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "class RL_Agents_Coord:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAf3qqa2bO-v"
      },
      "source": [
        "### Simulation environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-FoGj4CbdIG"
      },
      "source": [
        "# Loading libraries\n",
        "import sys\n",
        "sys.path.append(\"./CityLearn_garage\")\n",
        "\n",
        "from citylearn import CityLearn\n",
        "from reward_function import reward_function_ma\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from agent import RL_Agents_Coord\n",
        "\n",
        "import os\n",
        "import gym\n",
        "import numpy as np\n",
        "#from stable_baselines3 import SAC\n",
        "#from stable_baselines3.sac.policies import MlpPolicy as MlpPolicy_SAC\n",
        "#from stable_baselines3.common.callbacks import BaseCallback\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import pickle\n",
        "import copy"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPMFTC5icU0H"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ob8ccMUiawB3"
      },
      "source": [
        "def get_env(climate_zone):\n",
        "  # Load environment\n",
        "  data_path = Path(\"./CityLearn_garage/data/Climate_Zone_\"+str(climate_zone))\n",
        "  building_attributes = data_path / 'building_attributes.json'\n",
        "  weather_file = data_path / 'weather_data.csv'\n",
        "  solar_profile = data_path / 'solar_generation_1kW.csv'\n",
        "  building_state_actions = './CityLearn_garage/buildings_state_action_space.json'\n",
        "  building_ids = [\"Building_1\",\"Building_2\",\"Building_3\",\"Building_4\",\"Building_5\",\"Building_6\",\"Building_7\",\"Building_8\",\"Building_9\"]\n",
        "  objective_function = ['ramping','1-load_factor','average_daily_peak','peak_demand','net_electricity_consumption']\n",
        "\n",
        "  # Contain the lower and upper bounds of the states and actions, to be provided to the agent to normalize the variables between 0 and 1.\n",
        "  # Can be obtained using observations_spaces[i].low or .high\n",
        "  env = CityLearn(data_path, \n",
        "                  building_attributes, \n",
        "                  weather_file, \n",
        "                  solar_profile, \n",
        "                  building_ids, \n",
        "                  buildings_states_actions = building_state_actions, \n",
        "                  cost_function = objective_function, \n",
        "                  verbose = 1, \n",
        "                  simulation_period=(0,8760-1), \n",
        "                  central_agent=False)\n",
        "  # Provides information on Building type, Climate Zone, Annual DHW demand, Annual Cooling Demand, Annual Electricity Demand, Solar Capacity, and correllations among buildings\n",
        "  building_info = env.get_building_information()  \n",
        "  observations_spaces, actions_spaces = env.get_state_action_spaces()\n",
        "\n",
        "  return env, building_ids, building_state_actions, building_info, observations_spaces, actions_spaces"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUYuMEnAbOHp"
      },
      "source": [
        "env, building_ids, building_state_actions, building_info, observations_spaces, actions_spaces = get_env(1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDyZF_4bbXky",
        "outputId": "9ae865c7-8462-42a1-d011-bfbd51defaf7"
      },
      "source": [
        "observations_spaces"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Box(-2.200000047683716, 1058.469970703125, (26,), float32),\n",
              " Box(-2.200000047683716, 1058.469970703125, (26,), float32),\n",
              " Box(-2.200000047683716, 1058.469970703125, (25,), float32),\n",
              " Box(-2.200000047683716, 1058.469970703125, (26,), float32),\n",
              " Box(-2.200000047683716, 1058.469970703125, (26,), float32),\n",
              " Box(-2.200000047683716, 1058.469970703125, (26,), float32),\n",
              " Box(-2.200000047683716, 1058.469970703125, (26,), float32),\n",
              " Box(-2.200000047683716, 1058.469970703125, (26,), float32),\n",
              " Box(-2.200000047683716, 1058.469970703125, (26,), float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIqsgws-buw7",
        "outputId": "4569cc81-0211-4a51-d1d6-2bffed4ca7c7"
      },
      "source": [
        "actions_spaces"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Box(-0.3333333432674408, 0.3333333432674408, (2,), float32),\n",
              " Box(-0.3333333432674408, 0.3333333432674408, (2,), float32),\n",
              " Box(-0.3333333432674408, 0.3333333432674408, (1,), float32),\n",
              " Box(-0.3333333432674408, 0.3333333432674408, (1,), float32),\n",
              " Box(-0.3333333432674408, 0.3333333432674408, (2,), float32),\n",
              " Box(-0.3333333432674408, 0.3333333432674408, (2,), float32),\n",
              " Box(-0.3333333432674408, 0.3333333432674408, (2,), float32),\n",
              " Box(-0.3333333432674408, 0.3333333432674408, (2,), float32),\n",
              " Box(-0.3333333432674408, 0.3333333432674408, (2,), float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EVUcfAqbvoX"
      },
      "source": [
        "### Train a instance of RI_Agents_Coord\n",
        "* First in first, I confirm wether the original multiple agents works correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aocmH24bb21Q"
      },
      "source": [
        "# Hyperparameters\n",
        "bs = 256\n",
        "tau = 0.005\n",
        "gamma = 0.99\n",
        "lr = 0.0003\n",
        "hid = [256,256]\n",
        "\n",
        "n_episodes = 12"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7N1oIbob40n"
      },
      "source": [
        "# Instantiating the control agent(s)\n",
        "agents = RL_Agents_Coord(building_ids, \n",
        "                         building_state_actions, \n",
        "                         building_info, \n",
        "                         observations_spaces, \n",
        "                         actions_spaces, \n",
        "                         discount = gamma, \n",
        "                         batch_size = bs, \n",
        "                         replay_buffer_capacity = 1e5, \n",
        "                         regression_buffer_capacity = 12*8760, \n",
        "                         tau=tau, \n",
        "                         lr=lr, \n",
        "                         hidden_dim=hid, \n",
        "                         start_training=8760*3, \n",
        "                         exploration_period = 8760*3+1,  \n",
        "                         start_regression=8760, \n",
        "                         information_sharing = True, \n",
        "                         pca_compression = .95, \n",
        "                         action_scaling_coef=0.5, \n",
        "                         reward_scaling = 5., \n",
        "                         update_per_step = 1, \n",
        "                         iterations_as = 2)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPtvG4TUR68I"
      },
      "source": [
        "**You can skip here**. This cord just confirming the original agents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "8kBA0CDMb7KT",
        "outputId": "faf83d84-7c58-474c-8758-ce97d2cb90d0"
      },
      "source": [
        "cost_by_epoch = []\n",
        "# The number of episodes can be replaces by a stopping criterion (i.e. convergence of the average reward)\n",
        "start = time.time()\n",
        "for e in range(n_episodes): \n",
        "    is_evaluating = (e > 7) # Evaluate deterministic policy after 7 epochs\n",
        "    rewards = []\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    j = 0\n",
        "    action, coordination_vars = agents.select_action(state, deterministic=is_evaluating)    \n",
        "    while not done:\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        action_next, coordination_vars_next = agents.select_action(next_state, deterministic=is_evaluating)\n",
        "        agents.add_to_buffer(state, action, reward, next_state, done, coordination_vars, coordination_vars_next)\n",
        "\n",
        "        state = next_state\n",
        "        coordination_vars = coordination_vars_next\n",
        "        action = action_next\n",
        "\n",
        "    cost = env.cost()\n",
        "    cost_by_epoch.append(cost)\n",
        "    print('Loss -', cost, 'Simulation time (min) -',(time.time()-start)/60.0)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cumulated reward: -168981466.40687048\n",
            "Cost score: {'ramping': 1.1905503, '1-load_factor': 1.0473200900968902, 'average_daily_peak': 1.0626521, 'peak_demand': 1.2674795, 'net_electricity_consumption': 1.0554651, 'total': 1.1246934339129326}\n",
            "Loss - {'ramping': 1.1905503, '1-load_factor': 1.0473200900968902, 'average_daily_peak': 1.0626521, 'peak_demand': 1.2674795, 'net_electricity_consumption': 1.0554651, 'total': 1.1246934339129326} Simulation time (min) - 0.9682521859804789\n",
            "Cumulated reward: -170647247.9540467\n",
            "Cost score: {'ramping': 1.184989, '1-load_factor': 1.0372429432671828, 'average_daily_peak': 1.0647068, 'peak_demand': 1.2937548, 'net_electricity_consumption': 1.0558867, 'total': 1.1273160565336855}\n",
            "Loss - {'ramping': 1.184989, '1-load_factor': 1.0372429432671828, 'average_daily_peak': 1.0647068, 'peak_demand': 1.2937548, 'net_electricity_consumption': 1.0558867, 'total': 1.1273160565336855} Simulation time (min) - 5.1020169695218405\n",
            "Cumulated reward: -170121406.9253281\n",
            "Cost score: {'ramping': 1.1868753, '1-load_factor': 1.0491583948401042, 'average_daily_peak': 1.0633984, 'peak_demand': 1.2435291, 'net_electricity_consumption': 1.0554862, 'total': 1.1196894765907015}\n",
            "Loss - {'ramping': 1.1868753, '1-load_factor': 1.0491583948401042, 'average_daily_peak': 1.0633984, 'peak_demand': 1.2435291, 'net_electricity_consumption': 1.0554862, 'total': 1.1196894765907015} Simulation time (min) - 10.177652104695637\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-12dbe7950a32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0maction_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoordination_vars_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_evaluating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoordination_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoordination_vars_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/CityLearn_garage/agent.py\u001b[0m in \u001b[0;36madd_to_buffer\u001b[0;34m(self, states, actions, rewards, next_states, done, coordination_vars, coordination_vars_next)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_optimizer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m                     \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_optimizer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ldyfi_TgcAXR"
      },
      "source": [
        "## Create the baseline RI_MAML_Agents_Coord\n",
        "\n",
        "* Here, create RI_MAML_Agetnts, `agent_ind.py` in CityLearn\n",
        "* week: Feb 15 ~ 19, I remove unnecessary parts from RI_Agents_Coord and save it as the first version of RI_MAML_Agents.\n",
        "* week: Feb 22 ~ , before creating RI_MAML_Agents, I create RI_REINFORCE_Agents to checke my understanding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXoip-MndFVA"
      },
      "source": [
        "import os\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Normal\n",
        "from torch.optim import Adam\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.utils as utils\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "import gym\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import json\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "# torch.autograd.set_detect_anomaly(True)\n",
        "import time\n",
        "import math\n",
        "\n",
        "# set the path so that we could read created modules\n",
        "import sys\n",
        "\n",
        "# organized into different classes for editing easier\n",
        "from RBC_Agent import RBC_Agent\n",
        "from model import PolicyNetwork, SoftQNetwork, RegressionBuffer\n",
        "from ReplayBuffer import ReplayBuffer\n",
        "from data_process import no_normalization, periodic_normalization, onehot_encoding, normalize, remove_feature\n",
        "\n",
        "# check cuda is available\n",
        "if torch.cuda.is_available():\n",
        "    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "        "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UcdYWk6Tdb-"
      },
      "source": [
        "## Creating REINFORCE agents\n",
        "* MARISA reward, REINFORCE agent for each building, PCA and information shareing are removed.\n",
        "* *Reference*: [pytorch-REINFORCE](https://github.com/chingyaoc/pytorch-REINFORCE)\n",
        "\n",
        "    ![image](https://user-images.githubusercontent.com/56372825/108765699-1d8c3d80-7522-11eb-9c26-508e71c9fe08.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2wAQgOWNaN4"
      },
      "source": [
        "#### Policy network class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toRyRykm9QON"
      },
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self, hidden_size, num_inputs, action_space):\n",
        "        super(Policy, self).__init__()\n",
        "        self.action_space = action_space\n",
        "        num_outputs = action_space.shape[0]\n",
        "\n",
        "        self.linear1 = nn.Linear(num_inputs, hidden_size[0])\n",
        "        self.linear1h = nn.Linear(hidden_size[0], hidden_size[1])\n",
        "        self.linear2 = nn.Linear(hidden_size[1], num_outputs)\n",
        "        self.linear2_ = nn.Linear(hidden_size[1], num_outputs)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = inputs\n",
        "        x = F.relu(self.linear1(x))\n",
        "        x = F.relu(self.linear1h(x))\n",
        "        mu = self.linear2(x)\n",
        "        sigma_sq = self.linear2_(x)\n",
        "\n",
        "        return mu, sigma_sq"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZSwEGsbN4ie"
      },
      "source": [
        "pi = Variable(torch.FloatTensor([math.pi])).cuda()\n",
        "def normal(x, mu, sigma_sq):\n",
        "    a = (-1*(Variable(x)-mu).pow(2)/(2*sigma_sq)).exp()\n",
        "    b = 1/(2*sigma_sq*pi.expand_as(sigma_sq)).sqrt()\n",
        "    return a*b"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y69nnzFqNgHz"
      },
      "source": [
        "#### RI_REINFORCE_Agents class\n",
        "\n",
        "* REINFORCE updates Each policy for each buildings \n",
        "* Multiple buildings, MARISA reward, and no information sharing / PCA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUamOHae1Ji6"
      },
      "source": [
        "class RI_REINFORCE_Agents():\n",
        "    def __init__(self, building_ids, \n",
        "                 buildings_states_actions, \n",
        "                 building_info,\n",
        "                 observation_spaces = None, \n",
        "                 action_spaces = None):\n",
        "        \n",
        "        # Read valid state variables and action variables for each building\n",
        "        # they are written in buildings_state_action_space.json in CityLean\n",
        "        with open(buildings_states_actions) as json_file:\n",
        "            self.buildings_states_actions = json.load(json_file)\n",
        "\n",
        "        # Set seeds\n",
        "        seed = 20180517 # to init parameters\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        # Set action and ovservation spaces for each building\n",
        "        # e.g., 'Building_1' as uid\n",
        "        self.building_ids = building_ids \n",
        "        self.action_spaces = {uid : a_space for uid, a_space in zip(building_ids, action_spaces)}\n",
        "        self.observation_spaces = {uid : o_space for uid, o_space in zip(building_ids, observation_spaces)}\n",
        "    \n",
        "        # Initialize a policy and optimizers\n",
        "        # using empty dict, because keys, building_ids, are str.\n",
        "        self.policies, self.optimizers = {}, {}\n",
        "        for uid in self.building_ids:\n",
        "\n",
        "            # initialize plicy parameters of a policy\n",
        "            hidden_size = [400, 300]\n",
        "            num_inputs = self.observation_spaces[uid].shape[0]\n",
        "\n",
        "            # initialize a policy for a building\n",
        "            self.policies[uid] = Policy(hidden_size, num_inputs, self.action_spaces[uid])\n",
        "            self.policies[uid] = self.policies[uid].cuda()\n",
        "            # initialize a optimizer for a policy\n",
        "            self.optimizers[uid] = optim.Adam(self.policies[uid].parameters(), lr=1e-3)\n",
        "            # set the plicy in training mode\n",
        "            self.policies[uid].train()\n",
        "         \n",
        "    def select_action(self, states):\n",
        "        # states is a ndarray, so first chage it as dict of uid\n",
        "        states = {uid : o_space for uid, o_space in zip(self.building_ids, states)}\n",
        "        # initialize actions as dict\n",
        "        actions, log_probs, entropies = {}, {}, {}\n",
        "\n",
        "        # for each policy\n",
        "        for uid in self.building_ids:\n",
        "            # get a state for a bilding and convert it to torch.float32\n",
        "            state = torch.Tensor(states[uid])\n",
        "            mu, sigma_sq = self.policies[uid](Variable(state).cuda())\n",
        "            sigma_sq = F.softplus(sigma_sq)\n",
        "\n",
        "            eps = torch.randn(mu.size())\n",
        "            # calculate the probability\n",
        "            action = (mu + sigma_sq.sqrt()*Variable(eps).cuda()).data\n",
        "            \n",
        "            prob = normal(action, mu, sigma_sq)\n",
        "\n",
        "            entropy = -0.5*((sigma_sq+2*pi.expand_as(sigma_sq)).log()+1)\n",
        "            log_prob = prob.log()\n",
        "            \n",
        "            actions[uid] = action.detach().cpu().numpy()\n",
        "            entropies[uid] = entropy\n",
        "            log_probs[uid] = log_prob\n",
        "\n",
        "        # changes the type of actions from dict to list of np.ndarray\n",
        "        return list(actions.values()), list(log_probs.values()), list(entropies.values())\n",
        "\n",
        "    def update_policies(self, rewards, log_probs, entropies, gamma):\n",
        "        # for each policy\n",
        "        index = 0\n",
        "        for uid in self.building_ids:\n",
        "            # Dim of rewards is (number of steps) x (number of buildings)\n",
        "            # So take a one reward sequence (step 0 ~ 8759) for one building using uid\n",
        "            f = lambda x: x[index]\n",
        "            reward_one_building = [f(x) for x in rewards]\n",
        "            log_prob_one_building = [f(x) for x in log_probs]\n",
        "            entropy_one_building = [f(x) for x in entropies]\n",
        "            R = torch.zeros(1, )\n",
        "            loss = 0\n",
        "            for i in reversed(range(len(reward_one_building))):\n",
        "                R = gamma * R + reward_one_building[i]\n",
        "                loss = loss - (log_prob_one_building[i]*(Variable(R).expand_as(log_prob_one_building[i])).cuda()).sum() - (0.0001*entropy_one_building[i].cuda()).sum()\n",
        "            loss = loss / len(reward_one_building)\n",
        "\t\t\n",
        "            self.optimizers[uid].zero_grad()\n",
        "            loss.backward()\n",
        "            utils.clip_grad_norm(self.policies[uid].parameters(), 40)\n",
        "            self.optimizers[uid].step()\n",
        "\n",
        "            index += 1\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5LU2D4FNl1k"
      },
      "source": [
        "#### Checking how the REINFORCE agents work correctly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d7IoCvg_lV5"
      },
      "source": [
        "agent = RI_REINFORCE_Agents(building_ids, \n",
        "                             building_state_actions, \n",
        "                             building_info, \n",
        "                             observations_spaces, \n",
        "                             actions_spaces)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hY883XRVKPM9",
        "outputId": "147bfcb0-d74f-4a70-f1e7-f86827c6ca96"
      },
      "source": [
        "n_episodes = 12\n",
        "gamma = 0.99\n",
        "\n",
        "costs = []\n",
        "cumulated_rewards = []\n",
        "\n",
        "for i_episode in range(n_episodes):\n",
        "    print(f\"Episode:{i_episode}\")\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    entropies = []\n",
        "    log_probs = []\n",
        "    rewards = []\n",
        "    \n",
        "    while not done:\n",
        "        action, log_prob, entropy = agent.select_action(state)\n",
        "\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        entropies.append(entropy)\n",
        "        log_probs.append(log_prob)\n",
        "        rewards.append(reward)\n",
        "        state = next_state\n",
        "\n",
        "        # every step, update the parameters\n",
        "        # agent.update_policies(rewards, log_probs, entropies, gamma)\n",
        "        # entropies = []\n",
        "        # log_probs = []\n",
        "        # rewards = []           \n",
        "        \n",
        "        if done:\n",
        "            costs.append(env.cost())\n",
        "            cumulated_rewards.append(np.sum(rewards))\n",
        "            break\n",
        "    # Here is the first position of update_policies\n",
        "    agent.update_policies(rewards, log_probs, entropies, gamma)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode:0\n",
            "Cumulated reward: -197272960.87538075\n",
            "Cost score: {'ramping': 1.2480551, '1-load_factor': 1.1182843828045774, 'average_daily_peak': 1.1841508, 'peak_demand': 1.2459329, 'net_electricity_consumption': 1.049227, 'total': 1.1691300468413721}\n",
            "Episode:1\n",
            "Cumulated reward: -168369793.212318\n",
            "Cost score: {'ramping': 1.0726278, '1-load_factor': 1.094463854584757, 'average_daily_peak': 1.0670604, 'peak_demand': 1.29738, 'net_electricity_consumption': 1.0330071, 'total': 1.1129078208513385}\n",
            "Episode:2\n",
            "Cumulated reward: -160121322.6677437\n",
            "Cost score: {'ramping': 0.8927121, '1-load_factor': 1.0440851260273463, 'average_daily_peak': 1.0308814, 'peak_demand': 1.217092, 'net_electricity_consumption': 1.0353706, 'total': 1.04402825452981}\n",
            "Episode:3\n",
            "Cumulated reward: -159464957.52690515\n",
            "Cost score: {'ramping': 0.929452, '1-load_factor': 1.0424108860147443, 'average_daily_peak': 1.0374389, 'peak_demand': 1.198293, 'net_electricity_consumption': 1.0362958, 'total': 1.048778099959144}\n",
            "Episode:4\n",
            "Cumulated reward: -163395489.83682138\n",
            "Cost score: {'ramping': 1.0157409, '1-load_factor': 1.0531878130719254, 'average_daily_peak': 1.0371258, 'peak_demand': 1.1606455, 'net_electricity_consumption': 1.0367477, 'total': 1.0606895378646293}\n",
            "Episode:5\n",
            "Cumulated reward: -176491563.67216572\n",
            "Cost score: {'ramping': 1.1676898, '1-load_factor': 1.0888651533102074, 'average_daily_peak': 1.0576069, 'peak_demand': 1.2344091, 'net_electricity_consumption': 1.0375516, 'total': 1.1172245248789605}\n",
            "Episode:6\n",
            "Cumulated reward: -174343211.68865362\n",
            "Cost score: {'ramping': 1.1555697, '1-load_factor': 1.058067957734538, 'average_daily_peak': 1.0287267, 'peak_demand': 1.1881846, 'net_electricity_consumption': 1.03627, 'total': 1.0933637937258627}\n",
            "Episode:7\n",
            "Cumulated reward: -170632790.98399642\n",
            "Cost score: {'ramping': 1.0165869, '1-load_factor': 1.0488858331500983, 'average_daily_peak': 1.0251584, 'peak_demand': 1.2281212, 'net_electricity_consumption': 1.0427661, 'total': 1.0723036787760158}\n",
            "Episode:8\n",
            "Cumulated reward: -168366210.04185998\n",
            "Cost score: {'ramping': 0.85390157, '1-load_factor': 1.036400587664515, 'average_daily_peak': 1.0331162, 'peak_demand': 1.3553554, 'net_electricity_consumption': 1.0622853, 'total': 1.0682118120405018}\n",
            "Episode:9\n",
            "Cumulated reward: -169297610.90615538\n",
            "Cost score: {'ramping': 0.84863126, '1-load_factor': 1.0384867310559691, 'average_daily_peak': 1.035625, 'peak_demand': 1.5293034, 'net_electricity_consumption': 1.0669242, 'total': 1.1037941241271485}\n",
            "Episode:10\n",
            "Cumulated reward: -168804390.82577217\n",
            "Cost score: {'ramping': 0.8473054, '1-load_factor': 1.0507821692171875, 'average_daily_peak': 1.0375609, 'peak_demand': 1.4843122, 'net_electricity_consumption': 1.0662022, 'total': 1.0972325732935109}\n",
            "Episode:11\n",
            "Cumulated reward: -168574016.2799314\n",
            "Cost score: {'ramping': 0.84379435, '1-load_factor': 1.0435433501034321, 'average_daily_peak': 1.0374455, 'peak_demand': 1.4843122, 'net_electricity_consumption': 1.0660846, 'total': 1.0950360082393753}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVeED-6cb2-z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}