{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CityLearn_with_garage.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_OWi-NBKDBU"
      },
      "source": [
        "# CityLearn with Garage\n",
        "Name: Shota Takeshima\n",
        "* garage is a toolkit for developing and evaluating reinforcement learning algorithms, and an accompanying library of state-of-the-art implementations built using that toolkit.\n",
        "* First, I try to confirm SAC from garage can works with CityLearn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHm6rKTDNVYk"
      },
      "source": [
        "## Installation: Garage\n",
        "* garage supports python 3.6 or later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Az2QIx13NUZk",
        "outputId": "8267f637-31ce-4d87-eab0-975e99a5b147"
      },
      "source": [
        "!python --version"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.6.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3S6yn5VlKOjD",
        "outputId": "2823801c-ec20-4dcb-e8d3-e9158e24ed81"
      },
      "source": [
        "# install garage\n",
        "!echo \"abcd\" > mujoco_fake_key\n",
        "!rm -rf garage\n",
        "!git clone --depth 1 https://github.com/rlworkgroup/garage/\n",
        "# in this execution of batch script, an error occurs...\n",
        "!cd garage && bash scripts/setup_colab.sh --mjkey ../mujoco_fake_key --no-modify-bashrc > /dev/null!"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'garage'...\n",
            "remote: Enumerating objects: 755, done.\u001b[K\n",
            "remote: Counting objects: 100% (755/755), done.\u001b[K\n",
            "remote: Compressing objects: 100% (663/663), done.\u001b[K\n",
            "remote: Total 755 (delta 203), reused 220 (delta 78), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (755/755), 2.97 MiB | 30.39 MiB/s, done.\n",
            "Resolving deltas: 100% (203/203), done.\n",
            "start of setup_colab.sh\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 88.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "Cloning into '/tmp/tmp.3tpDeW1Re5/glfw'...\n",
            "remote: Enumerating objects: 61, done.\u001b[K\n",
            "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 26821 (delta 40), reused 49 (delta 35), pack-reused 26760\u001b[K\n",
            "Receiving objects: 100% (26821/26821), 13.07 MiB | 27.48 MiB/s, done.\n",
            "Resolving deltas: 100% (18914/18914), done.\n",
            "Note: checking out '0be4f3f75aebd9d24583ee86590a38e741db0904'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at 0be4f3f7 Add GLFW_FOCUS_ON_SHOW window hint and attribute\n",
            "--2021-02-05 17:01:20--  https://www.roboti.us/download/mujoco200_linux.zip\n",
            "Resolving www.roboti.us (www.roboti.us)... 104.40.85.93\n",
            "Connecting to www.roboti.us (www.roboti.us)|104.40.85.93|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4427362 (4.2M) [application/zip]\n",
            "Saving to: ‘/tmp/tmp.ZyvStx6Yc9/mujoco.zip’\n",
            "\n",
            "/tmp/tmp.ZyvStx6Yc9 100%[===================>]   4.22M  4.65MB/s    in 0.9s    \n",
            "\n",
            "2021-02-05 17:01:21 (4.65 MB/s) - ‘/tmp/tmp.ZyvStx6Yc9/mujoco.zip’ saved [4427362/4427362]\n",
            "\n",
            "\u001b[33m  WARNING: Missing build requirements in pyproject.toml for mujoco-py<2.1,>=2.0 from https://files.pythonhosted.org/packages/2f/48/b108057c1a23c8da9f4cdc7a7c46ab7cec49c3563c0706d50f2527de6ba0/mujoco-py-2.0.2.13.tar.gz#sha256=d6ae66276b565af9063597fda70683a89c7356290f5ac3961b794ee90ec50eea.\u001b[0m\n",
            "\u001b[33m  WARNING: The project does not specify a build backend, and pip cannot fall back to setuptools without 'wheel'.\u001b[0m\n",
            "  Running command git clone -q git://github.com/deepmind/dm_control.git /tmp/pip-req-build-haxah30j\n",
            "  Running command git rev-parse -q --verify 'sha^92f9913013face0468442cd0964d5973ea2089ea'\n",
            "  Running command git fetch -q git://github.com/deepmind/dm_control.git 92f9913013face0468442cd0964d5973ea2089ea\n",
            "  Running command git checkout -q 92f9913013face0468442cd0964d5973ea2089ea\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires coverage==3.7.1, but you have coverage 5.4 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "coveralls 0.5 requires coverage<3.999,>=3.6, but you have coverage 5.4 which is incompatible.\u001b[0m\n",
            "  Running command git clone -q https://github.com/rlworkgroup/metaworlds.git /tmp/pip-install-jgi1kgvs/metaworlds_3c53d86f4e724b239f19e329a64a16b6\n",
            "  Running command git clone -q https://github.com/rlworkgroup/viskit.git /tmp/pip-install-myxxov9k/viskit_90c836fda5a841c2b6d295d532cccd33\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cufflinks 0.17.3 requires plotly>=4.1.1, but you have plotly 1.9.6 which is incompatible.\u001b[0m\n",
            "end of setup_colab.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVCsykTbt3s9",
        "outputId": "d1d49ab4-9762-4f3e-f81a-3d852eb6fbd6"
      },
      "source": [
        "!garage examples"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-05 17:05:59.375722: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "# tf\n",
            "tf/ddpg_pendulum.py (garage.examples.tf.ddpg_pendulum)\n",
            "tf/dqn_cartpole.py (garage.examples.tf.dqn_cartpole)\n",
            "tf/dqn_pong.py (garage.examples.tf.dqn_pong)\n",
            "tf/erwr_cartpole.py (garage.examples.tf.erwr_cartpole)\n",
            "tf/her_ddpg_fetchreach.py (garage.examples.tf.her_ddpg_fetchreach)\n",
            "tf/multi_env_ppo.py (garage.examples.tf.multi_env_ppo)\n",
            "tf/multi_env_trpo.py (garage.examples.tf.multi_env_trpo)\n",
            "tf/ppo_memorize_digits.py (garage.examples.tf.ppo_memorize_digits)\n",
            "tf/ppo_pendulum.py (garage.examples.tf.ppo_pendulum)\n",
            "tf/reps_gym_cartpole.py (garage.examples.tf.reps_gym_cartpole)\n",
            "tf/resume_training.py (garage.examples.tf.resume_training)\n",
            "tf/rl2_ppo_halfcheetah.py (garage.examples.tf.rl2_ppo_halfcheetah)\n",
            "tf/rl2_ppo_halfcheetah_meta_test.py (garage.examples.tf.rl2_ppo_halfcheetah_meta_test)\n",
            "tf/rl2_ppo_metaworld_ml10.py (garage.examples.tf.rl2_ppo_metaworld_ml10)\n",
            "tf/rl2_ppo_metaworld_ml1_push.py (garage.examples.tf.rl2_ppo_metaworld_ml1_push)\n",
            "tf/rl2_ppo_metaworld_ml45.py (garage.examples.tf.rl2_ppo_metaworld_ml45)\n",
            "tf/rl2_trpo_halfcheetah.py (garage.examples.tf.rl2_trpo_halfcheetah)\n",
            "tf/td3_pendulum.py (garage.examples.tf.td3_pendulum)\n",
            "tf/te_ppo_metaworld_mt10.py (garage.examples.tf.te_ppo_metaworld_mt10)\n",
            "tf/te_ppo_metaworld_mt1_push.py (garage.examples.tf.te_ppo_metaworld_mt1_push)\n",
            "tf/te_ppo_metaworld_mt50.py (garage.examples.tf.te_ppo_metaworld_mt50)\n",
            "tf/te_ppo_point.py (garage.examples.tf.te_ppo_point)\n",
            "tf/trpo_cartpole.py (garage.examples.tf.trpo_cartpole)\n",
            "tf/trpo_cartpole_bullet.py (garage.examples.tf.trpo_cartpole_bullet)\n",
            "tf/trpo_cartpole_recurrent.py (garage.examples.tf.trpo_cartpole_recurrent)\n",
            "tf/trpo_cubecrash.py (garage.examples.tf.trpo_cubecrash)\n",
            "tf/trpo_gym_tf_cartpole.py (garage.examples.tf.trpo_gym_tf_cartpole)\n",
            "tf/trpo_gym_tf_cartpole_pretrained.py (garage.examples.tf.trpo_gym_tf_cartpole_pretrained)\n",
            "tf/trpo_swimmer.py (garage.examples.tf.trpo_swimmer)\n",
            "tf/trpo_swimmer_ray_sampler.py (garage.examples.tf.trpo_swimmer_ray_sampler)\n",
            "tf/tutorial_vpg.py (garage.examples.tf.tutorial_vpg)\n",
            "tf/vpg_cartpole.py (garage.examples.tf.vpg_cartpole)\n",
            "\n",
            "# torch\n",
            "torch/bc_point.py (garage.examples.torch.bc_point)\n",
            "torch/bc_point_deterministic_policy.py (garage.examples.torch.bc_point_deterministic_policy)\n",
            "torch/ddpg_pendulum.py (garage.examples.torch.ddpg_pendulum)\n",
            "torch/dqn_atari.py (garage.examples.torch.dqn_atari)\n",
            "torch/dqn_cartpole.py (garage.examples.torch.dqn_cartpole)\n",
            "torch/maml_ppo_half_cheetah_dir.py (garage.examples.torch.maml_ppo_half_cheetah_dir)\n",
            "torch/maml_trpo_half_cheetah_dir.py (garage.examples.torch.maml_trpo_half_cheetah_dir)\n",
            "torch/maml_trpo_metaworld_ml10.py (garage.examples.torch.maml_trpo_metaworld_ml10)\n",
            "torch/maml_trpo_metaworld_ml1_push.py (garage.examples.torch.maml_trpo_metaworld_ml1_push)\n",
            "torch/maml_trpo_metaworld_ml45.py (garage.examples.torch.maml_trpo_metaworld_ml45)\n",
            "torch/maml_vpg_half_cheetah_dir.py (garage.examples.torch.maml_vpg_half_cheetah_dir)\n",
            "torch/mtppo_metaworld_mt10.py (garage.examples.torch.mtppo_metaworld_mt10)\n",
            "torch/mtppo_metaworld_mt1_push.py (garage.examples.torch.mtppo_metaworld_mt1_push)\n",
            "torch/mtppo_metaworld_mt50.py (garage.examples.torch.mtppo_metaworld_mt50)\n",
            "torch/mtsac_metaworld_mt10.py (garage.examples.torch.mtsac_metaworld_mt10)\n",
            "torch/mtsac_metaworld_mt1_pick_place.py (garage.examples.torch.mtsac_metaworld_mt1_pick_place)\n",
            "torch/mtsac_metaworld_mt50.py (garage.examples.torch.mtsac_metaworld_mt50)\n",
            "torch/mttrpo_metaworld_mt10.py (garage.examples.torch.mttrpo_metaworld_mt10)\n",
            "torch/mttrpo_metaworld_mt1_push.py (garage.examples.torch.mttrpo_metaworld_mt1_push)\n",
            "torch/mttrpo_metaworld_mt50.py (garage.examples.torch.mttrpo_metaworld_mt50)\n",
            "torch/pearl_half_cheetah_vel.py (garage.examples.torch.pearl_half_cheetah_vel)\n",
            "torch/pearl_metaworld_ml10.py (garage.examples.torch.pearl_metaworld_ml10)\n",
            "torch/pearl_metaworld_ml1_push.py (garage.examples.torch.pearl_metaworld_ml1_push)\n",
            "torch/pearl_metaworld_ml45.py (garage.examples.torch.pearl_metaworld_ml45)\n",
            "torch/ppo_pendulum.py (garage.examples.torch.ppo_pendulum)\n",
            "torch/resume_training.py (garage.examples.torch.resume_training)\n",
            "torch/sac_half_cheetah_batch.py (garage.examples.torch.sac_half_cheetah_batch)\n",
            "torch/td3_halfcheetah.py (garage.examples.torch.td3_halfcheetah)\n",
            "torch/td3_pendulum.py (garage.examples.torch.td3_pendulum)\n",
            "torch/trpo_pendulum.py (garage.examples.torch.trpo_pendulum)\n",
            "torch/trpo_pendulum_ray_sampler.py (garage.examples.torch.trpo_pendulum_ray_sampler)\n",
            "torch/tutorial_vpg.py (garage.examples.torch.tutorial_vpg)\n",
            "torch/vpg_pendulum.py (garage.examples.torch.vpg_pendulum)\n",
            "torch/watch_atari.py (garage.examples.torch.watch_atari)\n",
            "\n",
            "# jupyter\n",
            "jupyter/custom_env.ipynb (garage.examples.jupyter.custom_env)\n",
            "jupyter/trpo_gym_tf_cartpole.ipynb (garage.examples.jupyter.trpo_gym_tf_cartpole)\n",
            "\n",
            "# np\n",
            "np/cem_cartpole.py (garage.examples.np.cem_cartpole)\n",
            "np/cma_es_cartpole.py (garage.examples.np.cma_es_cartpole)\n",
            "np/tutorial_cem.py (garage.examples.np.tutorial_cem)\n",
            "\n",
            "sim_policy.py (garage.examples.sim_policy)\n",
            "step_bullet_kuka_env.py (garage.examples.step_bullet_kuka_env)\n",
            "step_dm_control_env.py (garage.examples.step_dm_control_env)\n",
            "step_gym_env.py (garage.examples.step_gym_env)\n",
            "[6b0847c899b6:04108] *** Process received signal ***\n",
            "[6b0847c899b6:04108] Signal: Segmentation fault (11)\n",
            "[6b0847c899b6:04108] Signal code: Address not mapped (1)\n",
            "[6b0847c899b6:04108] Failing at address: 0x7fa3470b820d\n",
            "[6b0847c899b6:04108] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980)[0x7fa34a36d980]\n",
            "[6b0847c899b6:04108] [ 1] /lib/x86_64-linux-gnu/libc.so.6(getenv+0xa5)[0x7fa349fac8a5]\n",
            "[6b0847c899b6:04108] [ 2] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(_ZN13TCMallocGuardD1Ev+0x34)[0x7fa34a817e44]\n",
            "[6b0847c899b6:04108] [ 3] /lib/x86_64-linux-gnu/libc.so.6(__cxa_finalize+0xf5)[0x7fa349fad735]\n",
            "[6b0847c899b6:04108] [ 4] /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4(+0x13cb3)[0x7fa34a815cb3]\n",
            "[6b0847c899b6:04108] *** End of error message ***\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3w6rtYEP33G"
      },
      "source": [
        "**Restart this notebook here.** it's needed to recognize the installed packages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xXcoRQaXvCz"
      },
      "source": [
        "## Installation: CityLearn_garage\n",
        "\n",
        "To use CityLearn in garage framework, we need to modify CityLearn environment accoding to the [garage document](https://garage.readthedocs.io/en/latest/user/implement_env.html).\n",
        "\n",
        "* I modified the CityLearn env as CityLearn_garage and push it to my private git repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edOW6fSdX_g9",
        "outputId": "01cbaaa5-3038-4739-ad85-3f75853e29b7"
      },
      "source": [
        "!rm -rf ./CityLearn_garage/\n",
        "!git clone https://github.com/shttksm/CityLearn_garage.git"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'CityLearn_garage'...\n",
            "remote: Enumerating objects: 87, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/87)\u001b[K\rremote: Counting objects:   2% (2/87)\u001b[K\rremote: Counting objects:   3% (3/87)\u001b[K\rremote: Counting objects:   4% (4/87)\u001b[K\rremote: Counting objects:   5% (5/87)\u001b[K\rremote: Counting objects:   6% (6/87)\u001b[K\rremote: Counting objects:   8% (7/87)\u001b[K\rremote: Counting objects:   9% (8/87)\u001b[K\rremote: Counting objects:  10% (9/87)\u001b[K\rremote: Counting objects:  11% (10/87)\u001b[K\rremote: Counting objects:  12% (11/87)\u001b[K\rremote: Counting objects:  13% (12/87)\u001b[K\rremote: Counting objects:  14% (13/87)\u001b[K\rremote: Counting objects:  16% (14/87)\u001b[K\rremote: Counting objects:  17% (15/87)\u001b[K\rremote: Counting objects:  18% (16/87)\u001b[K\rremote: Counting objects:  19% (17/87)\u001b[K\rremote: Counting objects:  20% (18/87)\u001b[K\rremote: Counting objects:  21% (19/87)\u001b[K\rremote: Counting objects:  22% (20/87)\u001b[K\rremote: Counting objects:  24% (21/87)\u001b[K\rremote: Counting objects:  25% (22/87)\u001b[K\rremote: Counting objects:  26% (23/87)\u001b[K\rremote: Counting objects:  27% (24/87)\u001b[K\rremote: Counting objects:  28% (25/87)\u001b[K\rremote: Counting objects:  29% (26/87)\u001b[K\rremote: Counting objects:  31% (27/87)\u001b[K\rremote: Counting objects:  32% (28/87)\u001b[K\rremote: Counting objects:  33% (29/87)\u001b[K\rremote: Counting objects:  34% (30/87)\u001b[K\rremote: Counting objects:  35% (31/87)\u001b[K\rremote: Counting objects:  36% (32/87)\u001b[K\rremote: Counting objects:  37% (33/87)\u001b[K\rremote: Counting objects:  39% (34/87)\u001b[K\rremote: Counting objects:  40% (35/87)\u001b[K\rremote: Counting objects:  41% (36/87)\u001b[K\rremote: Counting objects:  42% (37/87)\u001b[K\rremote: Counting objects:  43% (38/87)\u001b[K\rremote: Counting objects:  44% (39/87)\u001b[K\rremote: Counting objects:  45% (40/87)\u001b[K\rremote: Counting objects:  47% (41/87)\u001b[K\rremote: Counting objects:  48% (42/87)\u001b[K\rremote: Counting objects:  49% (43/87)\u001b[K\rremote: Counting objects:  50% (44/87)\u001b[K\rremote: Counting objects:  51% (45/87)\u001b[K\rremote: Counting objects:  52% (46/87)\u001b[K\rremote: Counting objects:  54% (47/87)\u001b[K\rremote: Counting objects:  55% (48/87)\u001b[K\rremote: Counting objects:  56% (49/87)\u001b[K\rremote: Counting objects:  57% (50/87)\u001b[K\rremote: Counting objects:  58% (51/87)\u001b[K\rremote: Counting objects:  59% (52/87)\u001b[K\rremote: Counting objects:  60% (53/87)\u001b[K\rremote: Counting objects:  62% (54/87)\u001b[K\rremote: Counting objects:  63% (55/87)\u001b[K\rremote: Counting objects:  64% (56/87)\u001b[K\rremote: Counting objects:  65% (57/87)\u001b[K\rremote: Counting objects:  66% (58/87)\u001b[K\rremote: Counting objects:  67% (59/87)\u001b[K\rremote: Counting objects:  68% (60/87)\u001b[K\rremote: Counting objects:  70% (61/87)\u001b[K\rremote: Counting objects:  71% (62/87)\u001b[K\rremote: Counting objects:  72% (63/87)\u001b[K\rremote: Counting objects:  73% (64/87)\u001b[K\rremote: Counting objects:  74% (65/87)\u001b[K\rremote: Counting objects:  75% (66/87)\u001b[K\rremote: Counting objects:  77% (67/87)\u001b[K\rremote: Counting objects:  78% (68/87)\u001b[K\rremote: Counting objects:  79% (69/87)\u001b[K\rremote: Counting objects:  80% (70/87)\u001b[K\rremote: Counting objects:  81% (71/87)\u001b[K\rremote: Counting objects:  82% (72/87)\u001b[K\rremote: Counting objects:  83% (73/87)\u001b[K\rremote: Counting objects:  85% (74/87)\u001b[K\rremote: Counting objects:  86% (75/87)\u001b[K\rremote: Counting objects:  87% (76/87)\u001b[K\rremote: Counting objects:  88% (77/87)\u001b[K\rremote: Counting objects:  89% (78/87)\u001b[K\rremote: Counting objects:  90% (79/87)\u001b[K\rremote: Counting objects:  91% (80/87)\u001b[K\rremote: Counting objects:  93% (81/87)\u001b[K\rremote: Counting objects:  94% (82/87)\u001b[K\rremote: Counting objects:  95% (83/87)\u001b[K\rremote: Counting objects:  96% (84/87)\u001b[K\rremote: Counting objects:  97% (85/87)\u001b[K\rremote: Counting objects:  98% (86/87)\u001b[K\rremote: Counting objects: 100% (87/87)\u001b[K\rremote: Counting objects: 100% (87/87), done.\u001b[K\n",
            "remote: Compressing objects: 100% (80/80), done.\u001b[K\n",
            "remote: Total 87 (delta 6), reused 84 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (87/87), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-YzbRZaX_nK",
        "outputId": "f5fe73b7-08ea-4472-f377-dc6c5ceea3b4"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"./CityLearn_garage\")\n",
        "\n",
        "from citylearn import CityLearn\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "!pip install stable_baselines3\n",
        "\n",
        "import os\n",
        "import gym\n",
        "import numpy as np\n",
        "from stable_baselines3 import SAC\n",
        "from stable_baselines3.sac.policies import MlpPolicy as MlpPolicy_SAC\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import time\n",
        "import pandas as pd\n",
        "import seaborn as sns"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: stable_baselines3 in /usr/local/lib/python3.6/dist-packages (0.10.0)\n",
            "Requirement already satisfied: gym>=0.17 in /usr/local/lib/python3.6/dist-packages (from stable_baselines3) (0.17.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stable_baselines3) (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from stable_baselines3) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from stable_baselines3) (1.1.5)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from stable_baselines3) (1.7.0+cu101)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from stable_baselines3) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.17->stable_baselines3) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym>=0.17->stable_baselines3) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17->stable_baselines3) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->stable_baselines3) (0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->stable_baselines3) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable_baselines3) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable_baselines3) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable_baselines3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable_baselines3) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->stable_baselines3) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->stable_baselines3) (2018.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q69r7itxX_sR"
      },
      "source": [
        "# Select the climate zone and load environment\n",
        "climate_zone = 1\n",
        "data_path = Path(\"./CityLearn_garage/data/Climate_Zone_\"+str(climate_zone))\n",
        "building_attributes = data_path / 'building_attributes.json'\n",
        "weather_file = data_path / 'weather_data.csv'\n",
        "solar_profile = data_path / 'solar_generation_1kW.csv'\n",
        "building_state_actions = './CityLearn_garage/buildings_state_action_space.json'\n",
        "building_ids = [\"Building_1\",\"Building_2\",\"Building_3\",\"Building_4\",\"Building_5\",\"Building_6\",\"Building_7\",\"Building_8\",\"Building_9\"]\n",
        "objective_function = ['ramping','1-load_factor','average_daily_peak','peak_demand','net_electricity_consumption']\n",
        "env = CityLearn(data_path, \n",
        "                building_attributes, \n",
        "                weather_file, \n",
        "                solar_profile, \n",
        "                building_ids, \n",
        "                buildings_states_actions = building_state_actions, \n",
        "                cost_function = objective_function,\n",
        "                central_agent = True,\n",
        "                verbose=1)\n",
        "\n",
        "observations_spaces, actions_spaces = env.get_state_action_spaces()\n",
        "building_info = env.get_building_information() "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psj9EpRvcN8c"
      },
      "source": [
        "See buildings_state_action_space.json. Basically, there are 28 variables that composes the state. For example, Building_1 uses 26 variables out of 28 possible variables as its state. On the other hand, Building_3 has 25 elements that is one less than Building_1's. This is because Building_3 has only cooling_storage, not have dhw_storage. So, dhw_storage_soc is not a variable that composes Building_3's state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOrtOpXbX_vl",
        "outputId": "21573010-cd4f-4a26-de14-f60b986fb58f"
      },
      "source": [
        "observations_spaces"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Box(26,),\n",
              " Box(26,),\n",
              " Box(25,),\n",
              " Box(26,),\n",
              " Box(26,),\n",
              " Box(26,),\n",
              " Box(26,),\n",
              " Box(26,),\n",
              " Box(26,)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD2OJzljX_zI",
        "outputId": "9aafc3a6-058b-4205-f61b-54eda5eb9c90"
      },
      "source": [
        "actions_spaces"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Box(2,),\n",
              " Box(2,),\n",
              " Box(1,),\n",
              " Box(1,),\n",
              " Box(2,),\n",
              " Box(2,),\n",
              " Box(2,),\n",
              " Box(2,),\n",
              " Box(2,)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSbfWPLOjjxN"
      },
      "source": [
        "#building_info"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p66FEfD8KqoL"
      },
      "source": [
        "Above state and action spaces are for multi agents RI, below ones are for the single central agent. The number of variables included in the action space is the sum of `actions_spaces` for muliple agent. The number of variables included in the state space is not corresponding to the sum of `observations_spaces` because all building has the same variable like `t_out`, outdoor temperature. So, one of them is includes as a state variable, others are omitted.\n",
        "\n",
        "From `citylearn.py`\n",
        "```python\n",
        "# If the agent is centralized, we append the states avoiding repetition. I.e. if multiple buildings share the outdoor temperature as a state, we only append i\\t once to the states of the central agent. The variable s_appended is used for this purpose. \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQEbT0HAKolt",
        "outputId": "77a55403-cf46-48e7-d540-6a65aa89d425"
      },
      "source": [
        "env.action_space"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(16,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSBQEWNMKkF7",
        "outputId": "838d0692-a72a-4fe7-b18c-861a13186bb5"
      },
      "source": [
        "env.observation_space"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(81,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3RGvQFChyFE"
      },
      "source": [
        "## Modifications for garage\n",
        "This is memos to remember what I modified to the original source code.\n",
        "\n",
        "* The modification instruction is [here](https://garage.readthedocs.io/en/latest/user/implement_env.html).\n",
        "* However, CityLearn is already a subclass of gym.Env. First, I just use a wrapper class [GymEnv](https://github.com/rlworkgroup/garage/blob/master/src/garage/envs/gym_env.py) in garage.\n",
        "\n",
        "  ```python\n",
        "  import gym\n",
        "  ...\n",
        "  class CityLearn(gym.Env):   \n",
        "  ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEvQ0Zf1f7Uh"
      },
      "source": [
        "## Confirming if CityLearn works correctly\n",
        "**If you modify CityLearn to make it suited to garage, check if the CityLearn works similarly as before it is modified.**\n",
        "* confirm if my modifications are correct, using SAC from stable baseline3 packages\n",
        "* This is a result from the original CityLearn code. (after fixing the reward_function.py. See Appendix.)\n",
        "![image](https://user-images.githubusercontent.com/56372825/107001085-06de9c00-6757-11eb-8fe8-a2fa9bd642ce.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9jfc0sKX_2F",
        "outputId": "e5aefafa-465d-482d-ae04-2f579b7ca6bf"
      },
      "source": [
        "%%time\n",
        "env.reset()\n",
        "agent = SAC(MlpPolicy_SAC, env, verbose=0, learning_rate=0.001, gamma=0.99, tau=3e-4, batch_size=2048, learning_starts=8759)\n",
        "agent.learn(total_timesteps=8760 * 12, log_interval=1000)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cumulated reward: -113012696825.10913\n",
            "Cumulated reward: -106994741264.02734\n",
            "Cumulated reward: -109388770632.78522\n",
            "Cumulated reward: -101347014538.57733\n",
            "Cumulated reward: -92452868926.78209\n",
            "Cumulated reward: -89046026243.33894\n",
            "Cumulated reward: -88986360804.9545\n",
            "Cumulated reward: -90421942685.15973\n",
            "Cumulated reward: -94936514890.54431\n",
            "Cumulated reward: -91580495960.89917\n",
            "Cumulated reward: -93396848292.75578\n",
            "Cumulated reward: -90913042180.85068\n",
            "CPU times: user 24min 4s, sys: 49.9 s, total: 24min 54s\n",
            "Wall time: 25min 2s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytcnybapX_4q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27a5261c-4f26-4286-8b67-21722e631b56"
      },
      "source": [
        "obs = env.reset()\n",
        "dones = False\n",
        "counter = []\n",
        "while dones==False:\n",
        "    action, _states = agent.predict(obs)\n",
        "    obs, rewards, dones, info = env.step(action)\n",
        "    counter.append(rewards)\n",
        "env.cost()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cumulated reward: -91229984822.5988\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "./CityLearn_garage/citylearn.py:527: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  self.state = np.array(self.state)\n",
            "./CityLearn_garage/citylearn.py:41: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return np.array(a)\n",
            "./CityLearn_garage/citylearn.py:449: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  self.state = np.array(self.state)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'1-load_factor': 0.9533623752498285,\n",
              " 'average_daily_peak': 0.92366856,\n",
              " 'net_electricity_consumption': 1.0157526,\n",
              " 'peak_demand': 1.1265297,\n",
              " 'ramping': 0.859761,\n",
              " 'total': 0.9758148371677329}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZHyrt3gX_9Y"
      },
      "source": [
        "## CityLearn with garage\n",
        "\n",
        "### SAC : Pending Todo, clarify each functions and their parameters\n",
        "* referring to the [SAC example of garage](https://github.com/rlworkgroup/garage/blob/master/src/garage/examples/torch/sac_half_cheetah_batch.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5H80QJEEgIn"
      },
      "source": [
        "env.reset()\n",
        "pass"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sw-zrpH2X__1"
      },
      "source": [
        "\"\"\"This is an example to train a task with SAC algorithm written in PyTorch.\"\"\"\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from garage import wrap_experiment\n",
        "from garage.envs import GymEnv, normalize\n",
        "from garage.experiment import deterministic\n",
        "from garage.replay_buffer import PathBuffer\n",
        "from garage.sampler import FragmentWorker, LocalSampler\n",
        "from garage.torch import set_gpu_mode\n",
        "from garage.torch.algos import SAC\n",
        "from garage.torch.policies import TanhGaussianMLPPolicy, DeterministicMLPPolicy, GaussianMLPPolicy\n",
        "from garage.torch.q_functions import ContinuousMLPQFunction\n",
        "from garage.trainer import Trainer"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fcEPQC2YADE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d86450b6-f636-4faf-f5c1-9cf36ac0e429"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "@wrap_experiment(snapshot_mode='none')\n",
        "def sac_citylearn(ctxt=None, seed=1, env=None):\n",
        "    \"\"\"Set up environment and algorithm and run the task.\n",
        "    Args:\n",
        "        ctxt (garage.experiment.ExperimentContext): The experiment\n",
        "            configuration used by Trainer to create the snapshotter.\n",
        "        seed (int): Used to seed the random number generator to produce\n",
        "            determinism.\n",
        "    \"\"\"\n",
        "    deterministic.set_seed(seed)\n",
        "    trainer = Trainer(snapshot_config=ctxt)\n",
        "    # add the optional arg, max_episode_length\n",
        "    env = GymEnv(env, max_episode_length=env.simulation_period[1])\n",
        "\n",
        "    policy = TanhGaussianMLPPolicy(\n",
        "        env_spec=env.spec,\n",
        "        hidden_sizes=[256, 256],\n",
        "        hidden_nonlinearity=nn.ReLU,\n",
        "        output_nonlinearity=None,\n",
        "        min_std=np.exp(-20.),\n",
        "        max_std=np.exp(2.),\n",
        "    )\n",
        "\n",
        "    qf1 = ContinuousMLPQFunction(env_spec=env.spec,\n",
        "                                 hidden_sizes=[256, 256],\n",
        "                                 hidden_nonlinearity=F.relu)\n",
        "\n",
        "    qf2 = ContinuousMLPQFunction(env_spec=env.spec,\n",
        "                                 hidden_sizes=[256, 256],\n",
        "                                 hidden_nonlinearity=F.relu)\n",
        "\n",
        "    replay_buffer = PathBuffer(capacity_in_transitions=int(1e6))\n",
        "\n",
        "    sampler = LocalSampler(agents=policy,\n",
        "                           envs=env,\n",
        "                           max_episode_length=env.spec.max_episode_length,\n",
        "                           worker_class=FragmentWorker)\n",
        "\n",
        "    sac = SAC(env_spec=env.spec,\n",
        "              policy=policy,\n",
        "              qf1=qf1,\n",
        "              qf2=qf2,\n",
        "              sampler=sampler,\n",
        "              gradient_steps_per_itr=1000,\n",
        "              max_episode_length_eval=1000,\n",
        "              replay_buffer=replay_buffer,\n",
        "              min_buffer_size=1e4,\n",
        "              target_update_tau=5e-3,\n",
        "              discount=0.99,\n",
        "              buffer_batch_size=256,\n",
        "              reward_scale=1.,\n",
        "              steps_per_epoch=1)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        set_gpu_mode(True)\n",
        "    else:\n",
        "        set_gpu_mode(False)\n",
        "    sac.to()\n",
        "    trainer.setup(algo=sac, env=env)\n",
        "    trainer.train(n_epochs=10, batch_size=1000)\n",
        "\n",
        "sac_citylearn(seed=521, env=env)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-05 17:54:43 | [sac_citylearn] Logging to /content/data/local/experiment/sac_citylearn_5\n",
            "2021-02-05 17:56:00 | [sac_citylearn] Obtaining samples...\n",
            "2021-02-05 17:56:34 | [sac_citylearn] epoch #0 | Saving snapshot...\n",
            "2021-02-05 17:56:34 | [sac_citylearn] epoch #0 | Saved\n",
            "2021-02-05 17:56:34 | [sac_citylearn] epoch #0 | Time 34.00 s\n",
            "2021-02-05 17:56:34 | [sac_citylearn] epoch #0 | EpochTime 34.00 s\n",
            "----------------------------------  ---------------\n",
            "AlphaTemperature/mean                   1.38377\n",
            "Average/TrainAverageReturn             -3.27839e+06\n",
            "Evaluation/AverageDiscountedReturn     -3.3824e+08\n",
            "Evaluation/AverageReturn               -3.03338e+09\n",
            "Evaluation/Iteration                    0\n",
            "Evaluation/MaxReturn                   -3.03338e+09\n",
            "Evaluation/MinReturn                   -3.03338e+09\n",
            "Evaluation/NumEpisodes                 10\n",
            "Evaluation/StdReturn                    0\n",
            "Evaluation/TerminationRate              0\n",
            "Policy/Loss                             5.8663e+06\n",
            "QF/Qf1Loss                              1.78049e+13\n",
            "QF/Qf2Loss                              1.78328e+13\n",
            "ReplayBuffer/buffer_size            10000\n",
            "TotalEnvSteps                       10000\n",
            "----------------------------------  ---------------\n",
            "2021-02-05 17:56:59 | [sac_citylearn] epoch #1 | Saving snapshot...\n",
            "2021-02-05 17:56:59 | [sac_citylearn] epoch #1 | Saved\n",
            "2021-02-05 17:56:59 | [sac_citylearn] epoch #1 | Time 59.15 s\n",
            "2021-02-05 17:56:59 | [sac_citylearn] epoch #1 | EpochTime 25.14 s\n",
            "----------------------------------  ---------------\n",
            "AlphaTemperature/mean                   1.87708\n",
            "Average/TrainAverageReturn             -3.06175e+06\n",
            "Evaluation/AverageDiscountedReturn     -3.3824e+08\n",
            "Evaluation/AverageReturn               -3.03338e+09\n",
            "Evaluation/Iteration                    1\n",
            "Evaluation/MaxReturn                   -3.03338e+09\n",
            "Evaluation/MinReturn                   -3.03338e+09\n",
            "Evaluation/NumEpisodes                 10\n",
            "Evaluation/StdReturn                    0\n",
            "Evaluation/TerminationRate              0\n",
            "Policy/Loss                             1.76037e+07\n",
            "QF/Qf1Loss                              1.92168e+13\n",
            "QF/Qf2Loss                              1.92726e+13\n",
            "ReplayBuffer/buffer_size            11000\n",
            "TotalEnvSteps                       11000\n",
            "----------------------------------  ---------------\n",
            "2021-02-05 17:57:24 | [sac_citylearn] epoch #2 | Saving snapshot...\n",
            "2021-02-05 17:57:24 | [sac_citylearn] epoch #2 | Saved\n",
            "2021-02-05 17:57:24 | [sac_citylearn] epoch #2 | Time 84.67 s\n",
            "2021-02-05 17:57:24 | [sac_citylearn] epoch #2 | EpochTime 25.51 s\n",
            "----------------------------------  ---------------\n",
            "AlphaTemperature/mean                   2.53747\n",
            "Average/TrainAverageReturn             -3.24226e+06\n",
            "Evaluation/AverageDiscountedReturn     -3.3824e+08\n",
            "Evaluation/AverageReturn               -3.03338e+09\n",
            "Evaluation/Iteration                    2\n",
            "Evaluation/MaxReturn                   -3.03338e+09\n",
            "Evaluation/MinReturn                   -3.03338e+09\n",
            "Evaluation/NumEpisodes                 10\n",
            "Evaluation/StdReturn                    0\n",
            "Evaluation/TerminationRate              0\n",
            "Policy/Loss                             3.02063e+07\n",
            "QF/Qf1Loss                              2.22413e+13\n",
            "QF/Qf2Loss                              2.22761e+13\n",
            "ReplayBuffer/buffer_size            12000\n",
            "TotalEnvSteps                       12000\n",
            "----------------------------------  ---------------\n",
            "2021-02-05 17:57:50 | [sac_citylearn] epoch #3 | Saving snapshot...\n",
            "2021-02-05 17:57:50 | [sac_citylearn] epoch #3 | Saved\n",
            "2021-02-05 17:57:50 | [sac_citylearn] epoch #3 | Time 110.54 s\n",
            "2021-02-05 17:57:50 | [sac_citylearn] epoch #3 | EpochTime 25.86 s\n",
            "----------------------------------  ---------------\n",
            "AlphaTemperature/mean                   3.42693\n",
            "Average/TrainAverageReturn             -2.89471e+06\n",
            "Evaluation/AverageDiscountedReturn     -3.3824e+08\n",
            "Evaluation/AverageReturn               -3.03338e+09\n",
            "Evaluation/Iteration                    3\n",
            "Evaluation/MaxReturn                   -3.03338e+09\n",
            "Evaluation/MinReturn                   -3.03338e+09\n",
            "Evaluation/NumEpisodes                 10\n",
            "Evaluation/StdReturn                    0\n",
            "Evaluation/TerminationRate              0\n",
            "Policy/Loss                             4.24053e+07\n",
            "QF/Qf1Loss                              2.00087e+13\n",
            "QF/Qf2Loss                              2.00412e+13\n",
            "ReplayBuffer/buffer_size            13000\n",
            "TotalEnvSteps                       13000\n",
            "----------------------------------  ---------------\n",
            "2021-02-05 17:58:16 | [sac_citylearn] epoch #4 | Saving snapshot...\n",
            "2021-02-05 17:58:16 | [sac_citylearn] epoch #4 | Saved\n",
            "2021-02-05 17:58:16 | [sac_citylearn] epoch #4 | Time 136.11 s\n",
            "2021-02-05 17:58:16 | [sac_citylearn] epoch #4 | EpochTime 25.56 s\n",
            "----------------------------------  ---------------\n",
            "AlphaTemperature/mean                   4.62671\n",
            "Average/TrainAverageReturn             -2.7583e+06\n",
            "Evaluation/AverageDiscountedReturn     -3.3824e+08\n",
            "Evaluation/AverageReturn               -3.03338e+09\n",
            "Evaluation/Iteration                    4\n",
            "Evaluation/MaxReturn                   -3.03338e+09\n",
            "Evaluation/MinReturn                   -3.03338e+09\n",
            "Evaluation/NumEpisodes                 10\n",
            "Evaluation/StdReturn                    0\n",
            "Evaluation/TerminationRate              0\n",
            "Policy/Loss                             5.19353e+07\n",
            "QF/Qf1Loss                              2.81111e+13\n",
            "QF/Qf2Loss                              2.81432e+13\n",
            "ReplayBuffer/buffer_size            14000\n",
            "TotalEnvSteps                       14000\n",
            "----------------------------------  ---------------\n",
            "2021-02-05 17:58:41 | [sac_citylearn] epoch #5 | Saving snapshot...\n",
            "2021-02-05 17:58:41 | [sac_citylearn] epoch #5 | Saved\n",
            "2021-02-05 17:58:41 | [sac_citylearn] epoch #5 | Time 161.60 s\n",
            "2021-02-05 17:58:41 | [sac_citylearn] epoch #5 | EpochTime 25.48 s\n",
            "----------------------------------  ---------------\n",
            "AlphaTemperature/mean                   6.24583\n",
            "Average/TrainAverageReturn             -2.98607e+06\n",
            "Evaluation/AverageDiscountedReturn     -3.3824e+08\n",
            "Evaluation/AverageReturn               -3.03338e+09\n",
            "Evaluation/Iteration                    5\n",
            "Evaluation/MaxReturn                   -3.03338e+09\n",
            "Evaluation/MinReturn                   -3.03338e+09\n",
            "Evaluation/NumEpisodes                 10\n",
            "Evaluation/StdReturn                    0\n",
            "Evaluation/TerminationRate              0\n",
            "Policy/Loss                             6.31956e+07\n",
            "QF/Qf1Loss                              3.59734e+13\n",
            "QF/Qf2Loss                              3.59975e+13\n",
            "ReplayBuffer/buffer_size            15000\n",
            "TotalEnvSteps                       15000\n",
            "----------------------------------  ---------------\n",
            "2021-02-05 17:59:07 | [sac_citylearn] epoch #6 | Saving snapshot...\n",
            "2021-02-05 17:59:07 | [sac_citylearn] epoch #6 | Saved\n",
            "2021-02-05 17:59:07 | [sac_citylearn] epoch #6 | Time 187.19 s\n",
            "2021-02-05 17:59:07 | [sac_citylearn] epoch #6 | EpochTime 25.59 s\n",
            "----------------------------------  ---------------\n",
            "AlphaTemperature/mean                   8.43119\n",
            "Average/TrainAverageReturn             -3.15219e+06\n",
            "Evaluation/AverageDiscountedReturn     -3.3824e+08\n",
            "Evaluation/AverageReturn               -3.03338e+09\n",
            "Evaluation/Iteration                    6\n",
            "Evaluation/MaxReturn                   -3.03338e+09\n",
            "Evaluation/MinReturn                   -3.03338e+09\n",
            "Evaluation/NumEpisodes                 10\n",
            "Evaluation/StdReturn                    0\n",
            "Evaluation/TerminationRate              0\n",
            "Policy/Loss                             7.07371e+07\n",
            "QF/Qf1Loss                              2.6109e+14\n",
            "QF/Qf2Loss                              2.61175e+14\n",
            "ReplayBuffer/buffer_size            16000\n",
            "TotalEnvSteps                       16000\n",
            "----------------------------------  ---------------\n",
            "2021-02-05 17:59:33 | [sac_citylearn] epoch #7 | Saving snapshot...\n",
            "2021-02-05 17:59:33 | [sac_citylearn] epoch #7 | Saved\n",
            "2021-02-05 17:59:33 | [sac_citylearn] epoch #7 | Time 212.98 s\n",
            "2021-02-05 17:59:33 | [sac_citylearn] epoch #7 | EpochTime 25.78 s\n",
            "----------------------------------  ---------------\n",
            "AlphaTemperature/mean                  11.3808\n",
            "Average/TrainAverageReturn             -3.4726e+06\n",
            "Evaluation/AverageDiscountedReturn     -3.3824e+08\n",
            "Evaluation/AverageReturn               -3.03338e+09\n",
            "Evaluation/Iteration                    7\n",
            "Evaluation/MaxReturn                   -3.03338e+09\n",
            "Evaluation/MinReturn                   -3.03338e+09\n",
            "Evaluation/NumEpisodes                 10\n",
            "Evaluation/StdReturn                    0\n",
            "Evaluation/TerminationRate              0\n",
            "Policy/Loss                             8.36693e+07\n",
            "QF/Qf1Loss                              5.47403e+13\n",
            "QF/Qf2Loss                              5.49903e+13\n",
            "ReplayBuffer/buffer_size            17000\n",
            "TotalEnvSteps                       17000\n",
            "----------------------------------  ---------------\n",
            "2021-02-05 17:59:58 | [sac_citylearn] epoch #8 | Saving snapshot...\n",
            "2021-02-05 17:59:58 | [sac_citylearn] epoch #8 | Saved\n",
            "2021-02-05 17:59:58 | [sac_citylearn] epoch #8 | Time 238.72 s\n",
            "2021-02-05 17:59:58 | [sac_citylearn] epoch #8 | EpochTime 25.73 s\n",
            "----------------------------------  ---------------\n",
            "AlphaTemperature/mean                  15.3623\n",
            "Average/TrainAverageReturn             -3.51965e+06\n",
            "Evaluation/AverageDiscountedReturn     -3.3824e+08\n",
            "Evaluation/AverageReturn               -3.03338e+09\n",
            "Evaluation/Iteration                    8\n",
            "Evaluation/MaxReturn                   -3.03338e+09\n",
            "Evaluation/MinReturn                   -3.03338e+09\n",
            "Evaluation/NumEpisodes                 10\n",
            "Evaluation/StdReturn                    0\n",
            "Evaluation/TerminationRate              0\n",
            "Policy/Loss                             9.31942e+07\n",
            "QF/Qf1Loss                              6.05635e+13\n",
            "QF/Qf2Loss                              6.10361e+13\n",
            "ReplayBuffer/buffer_size            18000\n",
            "TotalEnvSteps                       18000\n",
            "----------------------------------  ---------------\n",
            "2021-02-05 18:00:24 | [sac_citylearn] epoch #9 | Saving snapshot...\n",
            "2021-02-05 18:00:24 | [sac_citylearn] epoch #9 | Saved\n",
            "2021-02-05 18:00:24 | [sac_citylearn] epoch #9 | Time 264.82 s\n",
            "2021-02-05 18:00:24 | [sac_citylearn] epoch #9 | EpochTime 26.10 s\n",
            "----------------------------------  ---------------\n",
            "AlphaTemperature/mean                  20.7373\n",
            "Average/TrainAverageReturn             -3.93714e+06\n",
            "Evaluation/AverageDiscountedReturn     -3.3824e+08\n",
            "Evaluation/AverageReturn               -3.03338e+09\n",
            "Evaluation/Iteration                    9\n",
            "Evaluation/MaxReturn                   -3.03338e+09\n",
            "Evaluation/MinReturn                   -3.03338e+09\n",
            "Evaluation/NumEpisodes                 10\n",
            "Evaluation/StdReturn                    0\n",
            "Evaluation/TerminationRate              0\n",
            "Policy/Loss                             1.08218e+08\n",
            "QF/Qf1Loss                              5.00481e+13\n",
            "QF/Qf2Loss                              5.07451e+13\n",
            "ReplayBuffer/buffer_size            19000\n",
            "TotalEnvSteps                       19000\n",
            "----------------------------------  ---------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUqiCPAREH5P"
      },
      "source": [
        "Todo: After training correctly -> How to use the trained policy: https://garage.readthedocs.io/en/latest/user/reuse_garage_policy.html\n",
        "\n",
        "### TRPO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNxkxXLGXj1o",
        "outputId": "6d0b4593-f509-4557-aa75-17cb2d9520b2"
      },
      "source": [
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from garage import wrap_experiment\n",
        "from garage.envs import GymEnv\n",
        "from garage.experiment.deterministic import set_seed\n",
        "from garage.sampler import LocalSampler\n",
        "from garage.torch.algos import TRPO\n",
        "from garage.torch.policies import GaussianMLPPolicy\n",
        "from garage.torch.value_functions import GaussianMLPValueFunction\n",
        "from garage.trainer import Trainer\n",
        "\n",
        "\n",
        "@wrap_experiment\n",
        "def trpo_citylearn(ctxt=None, seed=1, env=env):\n",
        "    \"\"\"Train TRPO with InvertedDoublePendulum-v2 environment.\n",
        "    Args:\n",
        "        ctxt (garage.experiment.ExperimentContext): The experiment\n",
        "            configuration used by Trainer to create the snapshotter.\n",
        "        seed (int): Used to seed the random number generator to produce\n",
        "            determinism.\n",
        "    \"\"\"\n",
        "    set_seed(seed)\n",
        "    env = GymEnv(env, max_episode_length=env.simulation_period[1])\n",
        "\n",
        "    trainer = Trainer(ctxt)\n",
        "\n",
        "    policy = GaussianMLPPolicy(env.spec,\n",
        "                               hidden_sizes=[256, 256],\n",
        "                               hidden_nonlinearity=F.relu,\n",
        "                               output_nonlinearity=None)\n",
        "\n",
        "    value_function = GaussianMLPValueFunction(env_spec=env.spec,\n",
        "                                              hidden_sizes=(256, 256),\n",
        "                                              hidden_nonlinearity=F.relu,\n",
        "                                              output_nonlinearity=None)\n",
        "\n",
        "    sampler = LocalSampler(agents=policy,\n",
        "                           envs=env,\n",
        "                           max_episode_length=env.spec.max_episode_length)\n",
        "\n",
        "    algo = TRPO(env_spec=env.spec,\n",
        "                policy=policy,\n",
        "                value_function=value_function,\n",
        "                sampler=sampler,\n",
        "                discount=0.99,\n",
        "                center_adv=False)\n",
        "\n",
        "    trainer.setup(algo, env)\n",
        "    trainer.train(n_epochs=10, batch_size=1000)\n",
        "\n",
        "\n",
        "trpo_citylearn(seed=1)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-05 10:06:05 | [trpo_citylearn] [trpo_citylearn] [trpo_citylearn] [trpo_citylearn] [trpo_citylearn] Logging to /content/data/local/experiment/trpo_citylearn_6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/garage/src/garage/experiment/deterministic.py:37: UserWarning: Enabeling deterministic mode in PyTorch can have a performance impact when using GPU.\n",
            "  'Enabeling deterministic mode in PyTorch can have a performance '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-02-05 10:06:22 | [trpo_citylearn] [trpo_citylearn] [trpo_citylearn] [trpo_citylearn] [trpo_citylearn] Obtaining samples...\n",
            "Cumulated reward: -116099218698.46945\n",
            "2021-02-05 10:06:36 | [trpo_citylearn] [trpo_citylearn] [trpo_citylearn] [trpo_citylearn] [trpo_citylearn] epoch #0 | Saving snapshot...\n",
            "2021-02-05 10:06:42 | [trpo_citylearn] [trpo_citylearn] [trpo_citylearn] [trpo_citylearn] [trpo_citylearn] epoch #0 | Saved\n",
            "2021-02-05 10:06:42 | [trpo_citylearn] [trpo_citylearn] [trpo_citylearn] [trpo_citylearn] [trpo_citylearn] epoch #0 | Time 20.34 s\n",
            "2021-02-05 10:06:42 | [trpo_citylearn] [trpo_citylearn] [trpo_citylearn] [trpo_citylearn] [trpo_citylearn] epoch #0 | EpochTime 20.34 s\n",
            "-----------------------------------  --------------\n",
            "Evaluation/AverageDiscountedReturn     -3.94019e+08\n",
            "Evaluation/AverageReturn               -1.16099e+11\n",
            "Evaluation/Iteration                    0\n",
            "Evaluation/MaxReturn                   -1.16099e+11\n",
            "Evaluation/MinReturn                   -1.16099e+11\n",
            "Evaluation/NumEpisodes                  1\n",
            "Evaluation/StdReturn                    0\n",
            "Evaluation/TerminationRate              0\n",
            "GaussianMLPPolicy/Entropy              22.703\n",
            "GaussianMLPPolicy/KL                    0.00998671\n",
            "GaussianMLPPolicy/KLBefore              0\n",
            "GaussianMLPPolicy/LossAfter             4.14966e+08\n",
            "GaussianMLPPolicy/LossBefore            4.44055e+08\n",
            "GaussianMLPPolicy/dLoss                 2.90892e+07\n",
            "GaussianMLPValueFunction/LossAfter      9.03438e+17\n",
            "GaussianMLPValueFunction/LossBefore     1.65538e+18\n",
            "GaussianMLPValueFunction/dLoss          7.51946e+17\n",
            "TotalEnvSteps                        8759\n",
            "-----------------------------------  --------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8E9ucLc6fYAI",
        "outputId": "281f3211-cdeb-4f2f-875f-0a4e8120a6c7"
      },
      "source": [
        "normalize"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "garage.envs.normalized_env.NormalizedEnv"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFm7WbTJZjD9"
      },
      "source": [
        "## Appendix\n",
        "### Reward for single central agentb:\n",
        "* I observed the cost score became worse as the reward increased or the number of training epochs increased.\n",
        "* In step function of CityLearn class: `building.current_net_electricity_demand` stores each building's demand. And, it is positive if its (solar) generation is greater than its consumption.\n",
        "\n",
        "```python\n",
        "# Adding loads from appliances and subtracting solar generation to the net electrical load of each building                                                       \n",
        "building_electric_demand = round(_electric_demand_cooling + _electric_demand_dhw + _non_shiftable_load - _solar_generation, 4)                                    \n",
        "\n",
        "# Electricity consumed by every building                                                                                                                          \n",
        "building.current_net_electricity_demand = building_electric_demand                                                                                                \n",
        "self.buildings_net_electricity_demand.append(-building_electric_demand) # >0 if solar generation > electricity consumption  \n",
        "```\n",
        "\n",
        "* In `reward_function.py`: let `buildings_net_electricity_demand` be [-10, -20]. This means there are two buildings and each buildings consumes more than its generation. So, this is a bad case.\n",
        "\n",
        "1. reward_ = reward_ = -np.array(electricity_demand).sum(): $- (-10 - 20) = 30$ \n",
        "2. reward_ = max(0, reward_): $\\max(0, 30) = 30$\n",
        "3. reward_ = reward_**3: $30^3 = 27000$\n",
        "\n",
        "??? On the other hand, let `buildings_net_electricity_demand` be [10, 20]. Clearly this is a good case!\n",
        "\n",
        "1. reward_ = reward_ = -np.array(electricity_demand).sum(): $- (10 + 20) = -30$ \n",
        "2. reward_ = max(0, reward_): $\\max(0, -30) = 0$\n",
        "3. reward_ = reward_**3: $0^3 = 0$\n",
        "\n",
        "まじで頭悪い．I'll modify this code.\n",
        "\n",
        "```python\n",
        "# Reward function for the single-agent (centralized) agent                                                                                                                        \n",
        "def reward_function_sa(electricity_demand):                                                                                                                                       \n",
        "                                                                                                                                                                                  \n",
        "    reward_ = -np.array(electricity_demand).sum()                                                                                                                                 \n",
        "    reward_ = max(0, reward_)                                                                                                                                                     \n",
        "    reward_ = reward_**3.0                                                                                                                                                        \n",
        "                                                                                                                                                                                  \n",
        "    return reward_    \n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QQZwH9x2Y0w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}