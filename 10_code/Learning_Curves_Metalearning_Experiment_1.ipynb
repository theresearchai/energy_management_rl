{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Learning Curves - Metalearning - Experiment 1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBnXaIFKfPRd",
        "outputId": "5fccec1d-a8b9-48fe-fd36-3bff57878675"
      },
      "source": [
        "# get CityLearn from github\r\n",
        "!rm -rf ./CityLearn/\r\n",
        "!git clone https://github.com/intelligent-environments-lab/CityLearn.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'CityLearn'...\n",
            "remote: Enumerating objects: 216, done.\u001b[K\n",
            "remote: Counting objects: 100% (216/216), done.\u001b[K\n",
            "remote: Compressing objects: 100% (172/172), done.\u001b[K\n",
            "remote: Total 953 (delta 68), reused 174 (delta 37), pack-reused 737\u001b[K\n",
            "Receiving objects: 100% (953/953), 36.71 MiB | 19.46 MiB/s, done.\n",
            "Resolving deltas: 100% (497/497), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f0Tm3vjfVIS",
        "outputId": "05c829e5-c5fe-4b11-9f47-d337e4364e0a"
      },
      "source": [
        "!pip install stable_baselines3"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stable_baselines3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/97/f6da6fcaa96934832c02acf95a32309cfa8646b010221f6c7a14bfcf40d0/stable_baselines3-0.11.1-py3-none-any.whl (152kB)\n",
            "\r\u001b[K     |██▏                             | 10kB 15.0MB/s eta 0:00:01\r\u001b[K     |████▎                           | 20kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 30kB 9.9MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 40kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 51kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 61kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 71kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 81kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 92kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 102kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 112kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 122kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 133kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 143kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 153kB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (1.8.0+cu101)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (1.19.5)\n",
            "Requirement already satisfied: gym>=0.17 in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (0.17.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (1.1.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (3.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->stable_baselines3) (3.7.4.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17->stable_baselines3) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17->stable_baselines3) (1.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable_baselines3) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->stable_baselines3) (2018.9)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable_baselines3) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable_baselines3) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable_baselines3) (1.3.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17->stable_baselines3) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->stable_baselines3) (1.15.0)\n",
            "Installing collected packages: stable-baselines3\n",
            "Successfully installed stable-baselines3-0.11.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lbc0158fVo4",
        "outputId": "2300a263-4621-4bb5-f212-282e2f2d5c33"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/gdrive')\r\n",
        "!ls /gdrive"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMYUQJaPfZLO"
      },
      "source": [
        "# Loading libraries\r\n",
        "import sys\r\n",
        "sys.path.append(\"./CityLearn\")\r\n",
        "\r\n",
        "from citylearn import CityLearn\r\n",
        "from reward_function import reward_function_ma\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from pathlib import Path\r\n",
        "import numpy as np\r\n",
        "from agent import RL_Agents_Coord\r\n",
        "\r\n",
        "import os\r\n",
        "import gym\r\n",
        "import numpy as np\r\n",
        "from stable_baselines3 import SAC\r\n",
        "from stable_baselines3.sac.policies import MlpPolicy as MlpPolicy_SAC\r\n",
        "from stable_baselines3.common.callbacks import BaseCallback\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from pathlib import Path\r\n",
        "import time\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import seaborn as sns\r\n",
        "\r\n",
        "import pickle\r\n",
        "import copy\r\n",
        "\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N_iJvSlfbGr"
      },
      "source": [
        "def get_env(climate_zone):\r\n",
        "  # Load environment\r\n",
        "  data_path = Path(\"./CityLearn/data/Climate_Zone_\"+str(climate_zone))\r\n",
        "  building_attributes = data_path / 'building_attributes.json'\r\n",
        "  weather_file = data_path / 'weather_data.csv'\r\n",
        "  solar_profile = data_path / 'solar_generation_1kW.csv'\r\n",
        "  building_state_actions = './CityLearn/buildings_state_action_space.json'\r\n",
        "  building_ids = [\"Building_1\",\"Building_2\",\"Building_3\",\"Building_4\",\"Building_5\",\"Building_6\",\"Building_7\",\"Building_8\",\"Building_9\"]\r\n",
        "  objective_function = ['ramping','1-load_factor','average_daily_peak','peak_demand','net_electricity_consumption']\r\n",
        "\r\n",
        "  # Contain the lower and upper bounds of the states and actions, to be provided to the agent to normalize the variables between 0 and 1.\r\n",
        "  # Can be obtained using observations_spaces[i].low or .high\r\n",
        "  env = CityLearn(data_path, \r\n",
        "                  building_attributes, \r\n",
        "                  weather_file, \r\n",
        "                  solar_profile, \r\n",
        "                  building_ids, \r\n",
        "                  buildings_states_actions = building_state_actions, \r\n",
        "                  cost_function = objective_function, \r\n",
        "                  verbose = 0, \r\n",
        "                  simulation_period=(0,8760-1), \r\n",
        "                  central_agent=False)\r\n",
        "  # Provides information on Building type, Climate Zone, Annual DHW demand, Annual Cooling Demand, Annual Electricity Demand, Solar Capacity, and correllations among buildings\r\n",
        "  building_info = env.get_building_information()  \r\n",
        "  observations_spaces, actions_spaces = env.get_state_action_spaces()\r\n",
        "\r\n",
        "  return env, building_ids, building_state_actions, building_info, observations_spaces, actions_spaces"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aC6eBhonfeVD"
      },
      "source": [
        "def train_agent(n_episodes = 4, warm_up = 2, cz = 4):\r\n",
        "\r\n",
        "  '''Returns an agent trained at a certain climate zone for a specified number of episodes\r\n",
        "  \r\n",
        "  Parameters\r\n",
        "  ----------\r\n",
        "  n_episodes: int\r\n",
        "    Number of episodes, including learning and evaluation\r\n",
        "  warm_up: int\r\n",
        "    Number of initial episodes where policy will not be evaluated.\r\n",
        "    For episodes after the warm_up period, policies will be evaluated deterministically\r\n",
        "  cz: int\r\n",
        "    Climate zone where the agent is to be trained.\r\n",
        "    Note that this climate zone bears no relationship with the same parameter in the evaluation function.\r\n",
        "    Thus, it is possible to train an agent in one climate zone and evaluate it on another climate zone.\r\n",
        "  \r\n",
        "  Returns\r\n",
        "  -------\r\n",
        "  An agent trained on the specified climate zone\r\n",
        "  \r\n",
        "  '''\r\n",
        "\r\n",
        "  # Hyperparameters\r\n",
        "  bs = 256\r\n",
        "  tau = 0.005\r\n",
        "  gamma = 0.99\r\n",
        "  lr = 0.0003\r\n",
        "  hid = [256,256]\r\n",
        "\r\n",
        "  #Create environment for specified climate zone\r\n",
        "  env, building_ids, building_state_actions, building_info, observations_spaces, actions_spaces = get_env(cz)\r\n",
        "\r\n",
        "  # Instantiating the control agent(s)\r\n",
        "  agents = RL_Agents_Coord(building_ids, \r\n",
        "                          building_state_actions, \r\n",
        "                          building_info, \r\n",
        "                          observations_spaces, \r\n",
        "                          actions_spaces, \r\n",
        "                          discount = gamma, \r\n",
        "                          batch_size = bs, \r\n",
        "                          replay_buffer_capacity = 1e5, \r\n",
        "                          regression_buffer_capacity = 12*8760, \r\n",
        "                          tau=tau, \r\n",
        "                          lr=lr, \r\n",
        "                          hidden_dim=hid, \r\n",
        "                          start_training=8760*3, \r\n",
        "                          exploration_period = 8760*3+1,  \r\n",
        "                          start_regression=8760, \r\n",
        "                          information_sharing = True, \r\n",
        "                          pca_compression = .95, \r\n",
        "                          action_scaling_coef=0.5, \r\n",
        "                          reward_scaling = 5., \r\n",
        "                          update_per_step = 1, \r\n",
        "                          iterations_as = 2)\r\n",
        "  \r\n",
        "  #Learn\r\n",
        "  cost_by_epoch = []\r\n",
        "\r\n",
        "  # The number of episodes can be replaces by a stopping criterion (i.e. convergence of the average reward)\r\n",
        "  print(f'Number of episodes: {n_episodes}')\r\n",
        "  start = time.time()\r\n",
        "\r\n",
        "  for e in range(n_episodes): \r\n",
        "      is_evaluating = (e > warm_up) # Evaluate deterministic policy after warm_up period (in epochs)\r\n",
        "      rewards = []\r\n",
        "      state = env.reset()\r\n",
        "      done = False\r\n",
        "\r\n",
        "      j = 0\r\n",
        "      action, coordination_vars = agents.select_action(state, deterministic=is_evaluating)    \r\n",
        "      while not done:\r\n",
        "\r\n",
        "        next_state, reward, done, _ = env.step(action)\r\n",
        "        action_next, coordination_vars_next = agents.select_action(next_state, deterministic=is_evaluating)\r\n",
        "        agents.add_to_buffer(state, action, reward, next_state, done, coordination_vars, coordination_vars_next)\r\n",
        "\r\n",
        "        state = next_state\r\n",
        "        coordination_vars = coordination_vars_next\r\n",
        "        action = action_next\r\n",
        "        \r\n",
        "      cost = env.cost()\r\n",
        "      cost_by_epoch.append(cost)\r\n",
        "      print(e,': Loss -', cost, 'Simulation time (min) -',(time.time()-start)/60.0)\r\n",
        "    \r\n",
        "  return(agents)\r\n",
        "  "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0I0UjNN2tfJu",
        "outputId": "38f850e4-8eef-4ade-f061-dd7b6f4740af"
      },
      "source": [
        "agent = train_agent()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of episodes: 4\n",
            "0 : Loss - {'ramping': 1.1921318, '1-load_factor': 1.0913021875795699, 'average_daily_peak': 1.0890987, 'peak_demand': 1.2048324, 'net_electricity_consumption': 1.036682, 'total': 1.1228094163214561} Simulation time (min) - 1.164693574110667\n",
            "1 : Loss - {'ramping': 1.1914583, '1-load_factor': 1.1020179201472837, 'average_daily_peak': 1.0933446, 'peak_demand': 1.1925836, 'net_electricity_consumption': 1.0373905, 'total': 1.123358973052284} Simulation time (min) - 6.18104662100474\n",
            "2 : Loss - {'ramping': 1.1853551, '1-load_factor': 1.1134984223883568, 'average_daily_peak': 1.0924349, 'peak_demand': 1.1995869, 'net_electricity_consumption': 1.0372198, 'total': 1.1256190007694804} Simulation time (min) - 12.392118767897289\n",
            "3 : Loss - {'ramping': 0.81367445, '1-load_factor': 1.0524643322181817, 'average_daily_peak': 0.99137455, 'peak_demand': 1.1775756, 'net_electricity_consumption': 0.9979965, 'total': 1.0066170863380455} Simulation time (min) - 63.28154309193293\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rdnS26Puw6h"
      },
      "source": [
        "def evaluate(cz, agent = copy.deepcopy(agent), n_episodes = 4):\r\n",
        "\r\n",
        "  '''\r\n",
        "  Evaluate policy on given climate zone.\r\n",
        "  This function gets a pre-trained agent such as the one outputed by function **train_agent** \r\n",
        "  and evaluates its loss function on any given climate zone, which may or may not be the same\r\n",
        "  in which the agent was trained in the first place.\r\n",
        "\r\n",
        "  Parameters:\r\n",
        "  ----------\r\n",
        "  cz: int\r\n",
        "    The climate zone where the agent's performance is to be evaluated\r\n",
        "  agent: agent\r\n",
        "    An agent whose performance is to be evaluated on a specified climate zone.\r\n",
        "    This can be the output of the **train_agent** function.\r\n",
        "  n_episodes: int\r\n",
        "    The number of episodes over which the evaluation is to be made\r\n",
        "\r\n",
        "  Returns:\r\n",
        "  -------\r\n",
        "  list\r\n",
        "  A list containing the values of the loss function evaluated in each epoch.\r\n",
        "  In other words, the y-values of the learning curve\r\n",
        "\r\n",
        "  '''\r\n",
        "\r\n",
        "  cost_by_epoch = []\r\n",
        "\r\n",
        "  #Create environment for specified climate zone\r\n",
        "  env, building_ids, building_state_actions, building_info, observations_spaces, actions_spaces = get_env(cz)\r\n",
        "\r\n",
        "\r\n",
        "  # The number of episodes can be replaces by a stopping criterion (i.e. convergence of the average reward)\r\n",
        "  print(f'Number of episodes: {n_episodes}')\r\n",
        "  start = time.time()\r\n",
        "\r\n",
        "  for e in range(n_episodes): \r\n",
        "      is_evaluating = True\r\n",
        "      rewards = []\r\n",
        "      state = env.reset()\r\n",
        "      done = False\r\n",
        "\r\n",
        "      j = 0\r\n",
        "      action, coordination_vars = agent.select_action(state, deterministic=is_evaluating)    \r\n",
        "      while not done:\r\n",
        "\r\n",
        "        next_state, reward, done, _ = env.step(action)\r\n",
        "        action_next, coordination_vars_next = agent.select_action(next_state, deterministic=is_evaluating)\r\n",
        "\r\n",
        "        state = next_state\r\n",
        "        coordination_vars = coordination_vars_next\r\n",
        "        action = action_next\r\n",
        "        \r\n",
        "      cost = env.cost()['total']\r\n",
        "      cost_by_epoch.append(cost)\r\n",
        "      print(e,': Loss -', cost, 'Simulation time (min) -',(time.time()-start)/60.0)\r\n",
        "    \r\n",
        "  return(cost_by_epoch)\r\n",
        "  \r\n",
        "\r\n",
        "  "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buarFd2By0jr",
        "outputId": "de71cea3-f5a2-4cb7-8afc-f1928f904851"
      },
      "source": [
        "#learning_curve_cz1 = evaluate(1)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of episodes: 4\n",
            "0 : Loss - 0.9274882806684678 Simulation time (min) - 4.806350135803223\n",
            "1 : Loss - 0.9258588066901243 Simulation time (min) - 9.327413423856099\n",
            "2 : Loss - 0.926935663809829 Simulation time (min) - 13.816188526153564\n",
            "3 : Loss - 0.9265175797757672 Simulation time (min) - 18.28842971722285\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8Lc6q-E2HnO",
        "outputId": "bb6341ca-710c-4a97-efe9-dfdec648c94b"
      },
      "source": [
        "n_cz = 4\r\n",
        "learning_curves = {str(cz): [] for cz in range(1,n_cz + 1)}\r\n",
        "\r\n",
        "for cz in range(1, n_cz + 1):\r\n",
        "  print(f'*** climate zone {cz} ***')\r\n",
        "  learning_curves[cz] = evaluate(cz)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** climate zone 1 ***\n",
            "Number of episodes: 4\n",
            "0 : Loss - 0.9289516131965658 Simulation time (min) - 4.8157745440800985\n",
            "1 : Loss - 0.9247851448575662 Simulation time (min) - 9.246354289849599\n",
            "2 : Loss - 0.9269425851711357 Simulation time (min) - 13.676181856791178\n",
            "3 : Loss - 0.926651797520919 Simulation time (min) - 18.02609800895055\n",
            "*** climate zone 2 ***\n",
            "Number of episodes: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-wJDmEFTU8l"
      },
      "source": [
        "for cz in range(1, n_cz + 1):\r\n",
        "  plt.plot(learning_curves_by_cz[str(cz)])\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}